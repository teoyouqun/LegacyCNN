{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chiny\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\chiny\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n",
      "SpeechBrain could not find any working torchaudio backend. Audio files may fail to load. Follow this link for instructions and troubleshooting: https://pytorch.org/audio/stable/index.html\n",
      "SpeechBrain could not find any working torchaudio backend. Audio files may fail to load. Follow this link for instructions and troubleshooting: https://pytorch.org/audio/stable/index.html\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "from python_lib.modules import *\n",
    "import torch\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ECAPA_TDNN(input_size = 2, channels=[8,8,8,8,16], lin_neurons=6, device = \"cuda\").to(\"cuda\")\n",
    "input_feats = torch.rand([10,2,64]).to(\"cuda\")\n",
    "output = model(input_feats)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entire ECAPA-TDNN Model overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial out:  torch.Size([10, 8, 64])\n",
      "SERes2_1 out:  torch.Size([10, 8, 64])\n",
      "SERes2_2 out:  torch.Size([10, 8, 64])\n",
      "SERes2_3 out:  torch.Size([10, 8, 64])\n",
      "mfa out:  torch.Size([10, 8, 64])\n",
      "asp out:  torch.Size([10, 32, 1])\n",
      "final out:  torch.Size([10, 6])\n"
     ]
    }
   ],
   "source": [
    "# Default Activation fn = Relu and group = 1\n",
    "\n",
    "initialblock = TDNNBlock(in_channels=2,\n",
    "                        out_channels=8,\n",
    "                        kernel_size=5,\n",
    "                        dilation= 1).to(\"cuda\")\n",
    "\n",
    "x0 = initialblock(input_feats)\n",
    "print(\"Initial out: \",x0.shape)\n",
    "\n",
    "seres2_1 = SERes2NetBlock(in_channels=8,\n",
    "                          out_channels=8, \n",
    "                          res2net_scale=8, \n",
    "                          se_channels=128, \n",
    "                          kernel_size=3, \n",
    "                          dilation=2).to(\"cuda\")\n",
    "\n",
    "x1 = seres2_1(x0)\n",
    "print(\"SERes2_1 out: \",x1.shape)\n",
    "\n",
    "seres2_2 = SERes2NetBlock(in_channels=8,\n",
    "                          out_channels=8, \n",
    "                          res2net_scale=8, \n",
    "                          se_channels=128, \n",
    "                          kernel_size=3, \n",
    "                          dilation=3).to(\"cuda\")\n",
    "\n",
    "x2 = seres2_2(x1)\n",
    "print(\"SERes2_2 out: \",x2.shape)\n",
    "\n",
    "seres2_3 = SERes2NetBlock(in_channels=8,\n",
    "                          out_channels=8, \n",
    "                          res2net_scale=8, \n",
    "                          se_channels=128, \n",
    "                          kernel_size=3, \n",
    "                          dilation=4).to(\"cuda\")\n",
    "\n",
    "x3 = seres2_3(x2)\n",
    "print(\"SERes2_3 out: \",x3.shape)\n",
    "\n",
    "mfa = TDNNBlock(in_channels=(8 * 3),\n",
    "                out_channels=16,\n",
    "                kernel_size=1,\n",
    "                dilation= 1).to(\"cuda\")\n",
    "\n",
    "x4 = mfa(torch.cat([x1,x2,x3], dim=1))\n",
    "print(\"mfa out: \",x3.shape)\n",
    "\n",
    "asp = AttentiveStatisticsPooling(16, 128, True).to(\"cuda\")\n",
    "x5 = asp(x4)\n",
    "x6 = nn.BatchNorm1d(16 * 2).to(\"cuda\")(x5)\n",
    "print(\"asp out: \",x6.shape)\n",
    "\n",
    "# Remember to x2 to account for the mean & std in asp\n",
    "final = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=16 * 2, out_features=6),\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "y = final(x6)\n",
    "print(\"final out: \",y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disecting SERes2NetBlock\n",
    "\n",
    "Since in_channel == out_channel, shortcut is not necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 8, 64])\n",
      "torch.Size([10, 8, 64])\n",
      "torch.Size([10, 8, 64])\n",
      "torch.Size([10, 8, 64])\n"
     ]
    }
   ],
   "source": [
    "tdnn1 = TDNNBlock(in_channels=8, \n",
    "                  out_channels=8, \n",
    "                  kernel_size=1, \n",
    "                  dilation=1).to(\"cuda\")\n",
    "\n",
    "z0 = tdnn1(x0)\n",
    "print(z0.shape)\n",
    "res2net = Res2NetBlock(in_channels=8, \n",
    "                       out_channels=8, \n",
    "                       scale=8, \n",
    "                       kernel_size=3, \n",
    "                       dilation=1).to(\"cuda\")\n",
    "\n",
    "z1 = res2net(z0)\n",
    "print(z1.shape)\n",
    "\n",
    "tdnn2 = TDNNBlock(in_channels=8, \n",
    "                  out_channels=8, \n",
    "                  kernel_size=1, \n",
    "                  dilation=1).to(\"cuda\")\n",
    "\n",
    "z2 = tdnn2(z1)\n",
    "print(z2.shape)\n",
    "\n",
    "se_block = SEBlock(in_channels=8, \n",
    "                   se_channels=128, \n",
    "                   out_channels=8).to(\"cuda\")\n",
    "\n",
    "z3 = se_block(z2)\n",
    "print((z3 + x0).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disecting Res2NetBlock\n",
    "\n",
    "Here Scale = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 64])\n",
      "torch.Size([10, 1, 64])\n"
     ]
    }
   ],
   "source": [
    "scale = 8\n",
    "in_channel = 8//scale # 1\n",
    "hidden_channel = 8//scale # 1\n",
    "\n",
    "block1 = TDNNBlock(in_channels=1, \n",
    "                  out_channels=1, \n",
    "                  kernel_size=3, \n",
    "                  dilation=1).to(\"cuda\")\n",
    "block2 = TDNNBlock(in_channels=1, \n",
    "                  out_channels=1, \n",
    "                  kernel_size=3, \n",
    "                  dilation=1).to(\"cuda\")\n",
    "block3 = TDNNBlock(in_channels=1, \n",
    "                  out_channels=1, \n",
    "                  kernel_size=3, \n",
    "                  dilation=1).to(\"cuda\")\n",
    "block4 = TDNNBlock(in_channels=1, \n",
    "                  out_channels=1, \n",
    "                  kernel_size=3, \n",
    "                  dilation=1).to(\"cuda\")\n",
    "block5 = TDNNBlock(in_channels=1, \n",
    "                  out_channels=1, \n",
    "                  kernel_size=3, \n",
    "                  dilation=1).to(\"cuda\")\n",
    "block6 = TDNNBlock(in_channels=1, \n",
    "                  out_channels=1, \n",
    "                  kernel_size=3, \n",
    "                  dilation=1).to(\"cuda\")\n",
    "block7 = TDNNBlock(in_channels=1, \n",
    "                  out_channels=1, \n",
    "                  kernel_size=3, \n",
    "                  dilation=1).to(\"cuda\")\n",
    "\n",
    "chunks = torch.chunk(z0, 8, 1)\n",
    "# for i in chunks:\n",
    "#     print(i.shape)\n",
    "\n",
    "b7 = block7(chunks[0])\n",
    "print(chunks[0].shape)\n",
    "print(b7.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 64])\n",
      "torch.Size([10, 1, 62])\n"
     ]
    }
   ],
   "source": [
    "from speechbrain.nnet.CNN import Conv1d as _Conv1d\n",
    "\n",
    "class Conv1d(_Conv1d):\n",
    "    \"\"\"1D convolution. Skip transpose is used to improve efficiency.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(skip_transpose=True, *args, **kwargs)\n",
    "\n",
    "c1 = Conv1d(in_channels=1,\n",
    "            out_channels=1,\n",
    "            kernel_size=3,\n",
    "            dilation=1,\n",
    "            groups=1).to(\"cuda\")\n",
    "\n",
    "c2 = nn.Conv1d(1,1,3,1,0,1).to(\"cuda\")\n",
    "\n",
    "print(c1(chunks[0]).shape)\n",
    "print(c2(chunks[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[ 0.4432, -0.0911,  0.3460]]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1.conv.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7059469  0.02095687 0.20115513 0.7384187  0.8176412  0.9115216\n",
      "  0.06660366 0.18270296 0.05061555 0.33684415 0.32200992 0.8139815\n",
      "  0.723168   0.31516498 0.8346511  0.8369219  0.7861878  0.97466403\n",
      "  0.50959474 0.38968223 0.35643953 0.9147974  0.62809086 0.1278435\n",
      "  0.01682985 0.43299687 0.7622769  0.22550815 0.15225643 0.89812785\n",
      "  0.15977705 0.76698905 0.6259831  0.32144946 0.95843834 0.41918075\n",
      "  0.27270186 0.21022105 0.00486428 0.99132216 0.83308816 0.42072564\n",
      "  0.71016216 0.335509   0.27486247 0.13003528 0.2520151  0.8119419\n",
      "  0.41944677 0.76316583 0.43021357 0.55894905 0.06570715 0.69191074\n",
      "  0.9400216  0.6120556  0.3643071  0.43062645 0.588386   0.41756064\n",
      "  0.6555585  0.03994203 0.70191425 0.29078406]\n",
      " [0.8398572  0.39364266 0.588493   0.6983525  0.22568667 0.48973483\n",
      "  0.4378035  0.26462638 0.04081935 0.567016   0.43965113 0.04061586\n",
      "  0.35243702 0.43891114 0.74298733 0.7218001  0.35451025 0.20746297\n",
      "  0.57086706 0.92940277 0.02750117 0.525816   0.76538366 0.25545323\n",
      "  0.05624759 0.8865353  0.7363474  0.47148472 0.4681881  0.45152658\n",
      "  0.5141371  0.84656227 0.9906432  0.45070863 0.6494758  0.05292511\n",
      "  0.22998786 0.87125397 0.14431536 0.7200621  0.57553494 0.97474223\n",
      "  0.3479635  0.49301225 0.06022543 0.4309985  0.8894963  0.40511447\n",
      "  0.9943067  0.70034325 0.26602674 0.05723339 0.7635791  0.7510897\n",
      "  0.13270772 0.953043   0.02972168 0.5268831  0.4356392  0.7573553\n",
      "  0.809567   0.799988   0.69515616 0.7731654 ]\n",
      " [0.7374951  0.19776374 0.8405762  0.363701   0.94358873 0.04005182\n",
      "  0.0587135  0.1848371  0.9137467  0.08327121 0.34509522 0.42037523\n",
      "  0.3254537  0.445769   0.8665092  0.7502255  0.6685512  0.4397213\n",
      "  0.9303228  0.6203965  0.8271552  0.5465158  0.728633   0.30118757\n",
      "  0.27831244 0.38301808 0.26589215 0.5370152  0.63097006 0.86593103\n",
      "  0.40179926 0.61520404 0.2814979  0.19567537 0.1684109  0.35499948\n",
      "  0.47106367 0.4259109  0.57143897 0.40223688 0.7415648  0.74209756\n",
      "  0.07731646 0.2581092  0.00580996 0.02613819 0.6574727  0.16334009\n",
      "  0.05418301 0.63636255 0.38950652 0.03776526 0.5868009  0.5597713\n",
      "  0.76436216 0.8484893  0.31752414 0.169886   0.37202507 0.10337955\n",
      "  0.55259764 0.3488474  0.9688972  0.93005544]\n",
      " [0.73242533 0.72165465 0.40877146 0.24902105 0.14444941 0.4523487\n",
      "  0.7578553  0.73064035 0.7666115  0.5046694  0.65966564 0.833523\n",
      "  0.04172403 0.87968713 0.76687217 0.49806195 0.2783075  0.02491242\n",
      "  0.29413933 0.573369   0.6586512  0.6287397  0.17753077 0.3459047\n",
      "  0.22900164 0.9640524  0.5937051  0.4675876  0.9775422  0.7239386\n",
      "  0.5708055  0.10755551 0.08176029 0.13688737 0.7639968  0.31549215\n",
      "  0.12509882 0.4663334  0.7037318  0.46279335 0.0629046  0.17607844\n",
      "  0.9723297  0.3423183  0.6916498  0.6979662  0.6515371  0.41287476\n",
      "  0.5975287  0.32675052 0.67972976 0.3977335  0.4197932  0.9946428\n",
      "  0.79637104 0.5303526  0.283221   0.13396424 0.9109397  0.55404735\n",
      "  0.65748775 0.53579473 0.57073534 0.79149324]\n",
      " [0.7680814  0.6737666  0.4244762  0.4258352  0.8988039  0.7541534\n",
      "  0.5292538  0.67134106 0.77876776 0.24086219 0.1506049  0.3764817\n",
      "  0.10306311 0.38876122 0.73878205 0.18090993 0.33651048 0.92653745\n",
      "  0.30162996 0.41890717 0.6723299  0.8667253  0.777648   0.79034406\n",
      "  0.5208393  0.8220901  0.700952   0.97421    0.7976145  0.6443408\n",
      "  0.14969933 0.54319715 0.02841324 0.57728463 0.9761644  0.14302492\n",
      "  0.4241597  0.24037588 0.9088954  0.44937706 0.5468304  0.40490472\n",
      "  0.39086503 0.5665973  0.0257383  0.84750944 0.32259095 0.55707675\n",
      "  0.3145988  0.28206146 0.60845417 0.28730202 0.9113911  0.7941008\n",
      "  0.5594949  0.41899264 0.11767614 0.99491507 0.00165069 0.7848898\n",
      "  0.38924694 0.68825746 0.74304974 0.09830683]\n",
      " [0.498132   0.7867523  0.6343803  0.18210989 0.86953014 0.41632748\n",
      "  0.62940526 0.8616773  0.27392632 0.04223585 0.8697208  0.22317755\n",
      "  0.16954345 0.18710417 0.4293074  0.7084572  0.03680873 0.82942283\n",
      "  0.3566063  0.7526293  0.6306108  0.41211444 0.97047645 0.47465384\n",
      "  0.866482   0.9708964  0.5630541  0.3860122  0.91065866 0.92248756\n",
      "  0.7380919  0.9326917  0.69545394 0.72715455 0.45385247 0.67367315\n",
      "  0.52813405 0.22690606 0.03099459 0.47094166 0.68397695 0.5911416\n",
      "  0.78240716 0.46593946 0.12254035 0.713232   0.21203959 0.31469756\n",
      "  0.5750834  0.56333435 0.4105059  0.20438713 0.9620569  0.9054027\n",
      "  0.97041124 0.4407403  0.5289788  0.6152335  0.8074281  0.7315289\n",
      "  0.24136549 0.03399885 0.8208485  0.450733  ]\n",
      " [0.35812128 0.94544137 0.08394569 0.31496322 0.52610874 0.66229814\n",
      "  0.06696349 0.92320526 0.9851466  0.5427596  0.87239075 0.05142212\n",
      "  0.9545664  0.78650117 0.9466544  0.17416829 0.4832276  0.8633923\n",
      "  0.7996987  0.03183168 0.00658762 0.9625046  0.2904958  0.20533597\n",
      "  0.62080884 0.17500603 0.89562863 0.3253129  0.01067662 0.35845876\n",
      "  0.57644755 0.9239555  0.93497497 0.5644577  0.65997356 0.33384043\n",
      "  0.8195776  0.8526768  0.4774037  0.60668445 0.51242703 0.09424961\n",
      "  0.41313893 0.22778344 0.48838216 0.39407408 0.50113463 0.15769178\n",
      "  0.13165933 0.21373934 0.38550997 0.43520397 0.80376655 0.9168804\n",
      "  0.44001698 0.6509188  0.24572945 0.5860522  0.64431036 0.2486121\n",
      "  0.6081275  0.86202526 0.04248035 0.23643029]\n",
      " [0.98157364 0.2456854  0.65981084 0.6372041  0.0656268  0.37851256\n",
      "  0.7388317  0.8289809  0.27984822 0.36217886 0.15369105 0.8040812\n",
      "  0.73712087 0.38139367 0.7866366  0.23619235 0.15409571 0.71514463\n",
      "  0.12444854 0.31689698 0.7179732  0.8006724  0.7931739  0.60126317\n",
      "  0.8689178  0.2163502  0.05153644 0.13485575 0.20717448 0.12413889\n",
      "  0.93574417 0.3678196  0.18472463 0.4087878  0.28421462 0.23482406\n",
      "  0.25451893 0.9540959  0.44214112 0.85203964 0.02759206 0.65625703\n",
      "  0.3334549  0.75397587 0.88824666 0.7138888  0.83353275 0.11290026\n",
      "  0.47081745 0.8478475  0.8911697  0.09152162 0.35814875 0.40992564\n",
      "  0.98469883 0.4007007  0.3501532  0.8250587  0.01472789 0.02352297\n",
      "  0.48779958 0.0406242  0.5592173  0.1571278 ]]\n"
     ]
    }
   ],
   "source": [
    "input_feats = torch.rand([1,1,64]).to(\"cuda\")\n",
    "input_feats_np = input_feats.cpu().detach().numpy()\n",
    "input_feats_np = np.reshape(input_feats_np, (1,64))\n",
    "print(input_feats_np)\n",
    "dim = input_feats_np.shape\n",
    "\n",
    "flatten_inputs = input_feats_np.flatten()\n",
    "with open(os.path.join(\"ECAPAweights\", f\"inputseres2_1.bin\"), \"wb\") as f:\n",
    "            # Write the dimensions down\n",
    "            f.write(np.array(dim, dtype=np.int32).tobytes())\n",
    "            # Write the flatten bias down\n",
    "            f.write(flatten_inputs.tobytes())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
