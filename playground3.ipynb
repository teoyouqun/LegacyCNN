{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chiny\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\chiny\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n",
      "SpeechBrain could not find any working torchaudio backend. Audio files may fail to load. Follow this link for instructions and troubleshooting: https://pytorch.org/audio/stable/index.html\n",
      "SpeechBrain could not find any working torchaudio backend. Audio files may fail to load. Follow this link for instructions and troubleshooting: https://pytorch.org/audio/stable/index.html\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "from python_lib.modules import *\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from python_lib.saveasfile import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.14627564e-01 5.19425094e-01 9.03343558e-02 2.71279633e-01\n",
      "  8.00896764e-01 3.75588000e-01 5.82802892e-01 1.87831163e-01\n",
      "  2.32388556e-01 9.17293429e-02 4.29272354e-01 6.78330421e-01\n",
      "  2.23407269e-01 8.05796027e-01 4.49194074e-01 3.51586282e-01\n",
      "  9.33311582e-02 5.18459916e-01 4.18873072e-01 7.81540215e-01\n",
      "  9.67993081e-01 3.93763304e-01 5.37199974e-02 5.79132140e-01\n",
      "  5.18599033e-01 7.23363400e-01 2.06866503e-01 9.69597876e-01\n",
      "  4.67864871e-02 6.78856671e-01 4.34937716e-01 3.22561502e-01\n",
      "  7.60028422e-01 9.14776325e-02 3.87083709e-01 1.37053192e-01\n",
      "  1.45381272e-01 4.21477139e-01 7.02052832e-01 7.01632261e-01\n",
      "  2.86228240e-01 8.85494828e-01 4.20499444e-01 2.14118063e-01\n",
      "  6.00423813e-02 1.92389250e-01 5.15105903e-01 7.87610948e-01\n",
      "  9.68760490e-01 5.26328802e-01 4.35471177e-01 8.07291269e-01\n",
      "  2.04085708e-02 6.19444549e-01 1.12393379e-01 4.90337968e-01\n",
      "  2.40671575e-01 8.64619732e-01 9.69111919e-03 7.93832123e-01\n",
      "  5.46234250e-01 5.91592193e-01 7.78553843e-01 8.69101167e-01]\n",
      " [4.40722167e-01 7.97863543e-01 7.36706734e-01 2.58722782e-01\n",
      "  8.62274349e-01 5.64116359e-01 8.08024645e-01 4.61643338e-02\n",
      "  4.67886090e-01 1.36327744e-01 5.95946252e-01 2.53524184e-02\n",
      "  3.25574636e-01 7.71559060e-01 9.21370804e-01 3.95315647e-01\n",
      "  7.69814312e-01 8.74218345e-01 8.19629848e-01 6.66637421e-02\n",
      "  2.15724111e-01 7.28594065e-01 3.37107778e-02 1.38180077e-01\n",
      "  9.02222514e-01 1.16628587e-01 3.81898403e-01 5.86474657e-01\n",
      "  3.55354190e-01 5.96350789e-01 3.58041823e-01 5.03828287e-01\n",
      "  7.35870361e-01 4.77129757e-01 4.06195581e-01 4.58031714e-01\n",
      "  5.90250373e-01 2.37043262e-01 2.70765066e-01 8.96922886e-01\n",
      "  1.01888001e-01 8.18254471e-01 5.96692204e-01 4.88257110e-01\n",
      "  6.95540607e-01 1.08499050e-01 5.90136707e-01 9.55894649e-01\n",
      "  4.32521999e-01 7.08413064e-01 4.99611974e-01 9.35901642e-01\n",
      "  8.02745283e-01 2.68802285e-01 3.24532390e-01 8.85782182e-01\n",
      "  1.24875188e-01 6.49062514e-01 5.38769603e-01 3.04751635e-01\n",
      "  1.93210363e-01 6.67771339e-01 3.91229510e-01 3.77118409e-01]\n",
      " [3.53197575e-01 1.79858208e-02 8.24995160e-01 8.06224823e-01\n",
      "  9.00529027e-02 6.19257987e-01 7.94213235e-01 5.55284798e-01\n",
      "  5.14351904e-01 4.57184434e-01 2.14574575e-01 6.43287361e-01\n",
      "  8.72703850e-01 2.81660378e-01 5.54740429e-02 8.01026821e-02\n",
      "  9.91671681e-02 2.26930380e-01 2.27348745e-01 1.11828804e-01\n",
      "  3.97873580e-01 7.46465743e-01 3.79971027e-01 3.29998136e-01\n",
      "  3.98172259e-01 5.72772324e-01 1.39144897e-01 1.28166258e-01\n",
      "  3.56920958e-01 4.90022004e-01 6.22822881e-01 3.92492652e-01\n",
      "  2.41633356e-01 9.57987249e-01 8.42568994e-01 3.82312417e-01\n",
      "  1.85514331e-01 8.29632998e-01 2.05373168e-02 9.84405875e-02\n",
      "  8.49782109e-01 5.14963031e-01 9.18876350e-01 8.21768105e-01\n",
      "  5.94155014e-01 7.10155308e-01 6.59761548e-01 2.93000877e-01\n",
      "  6.08895957e-01 8.63167286e-01 5.05033493e-01 9.13781345e-01\n",
      "  7.97572196e-01 3.99738729e-01 2.13373721e-01 4.77589369e-02\n",
      "  8.01167309e-01 4.05302763e-01 4.83460665e-01 5.07012546e-01\n",
      "  3.29822540e-01 7.29806185e-01 1.62895560e-01 7.97417700e-01]\n",
      " [4.86117542e-01 5.33542931e-01 1.25192165e-01 5.78559935e-01\n",
      "  9.33512866e-01 5.68332076e-02 6.20779037e-01 4.93377447e-01\n",
      "  7.61273026e-01 5.53325295e-01 4.58077490e-01 5.05577266e-01\n",
      "  9.54599798e-01 2.68599570e-01 3.99091065e-01 4.80410576e-01\n",
      "  7.01703310e-01 8.24413896e-01 9.22014296e-01 5.96584737e-01\n",
      "  8.17582190e-01 2.83877552e-01 4.97629642e-01 8.01030815e-01\n",
      "  2.48820186e-02 9.72770214e-01 7.86610246e-01 4.49015498e-01\n",
      "  4.74735260e-01 8.38979483e-01 7.89228082e-02 1.64967835e-01\n",
      "  3.17942858e-01 8.29713643e-01 9.06650901e-01 5.35238743e-01\n",
      "  7.23979712e-01 8.41582835e-01 2.91263461e-01 3.89294684e-01\n",
      "  6.85785234e-01 6.00189686e-01 2.94760942e-01 2.25277126e-01\n",
      "  5.03605723e-01 9.44651425e-01 8.80011976e-01 3.12020242e-01\n",
      "  9.06140864e-01 3.12566757e-01 9.92728293e-01 9.91397321e-01\n",
      "  3.42932463e-01 3.27120662e-01 3.11616719e-01 7.63785720e-01\n",
      "  5.06060779e-01 8.01854372e-01 8.09667110e-02 7.38797784e-01\n",
      "  8.61304879e-01 3.98822129e-01 7.50176728e-01 9.58041072e-01]\n",
      " [2.48581350e-01 1.11996651e-01 4.36165452e-01 8.78766537e-01\n",
      "  4.39899147e-01 2.82605767e-01 3.70163739e-01 2.49987841e-03\n",
      "  3.66847694e-01 7.35419452e-01 1.86251879e-01 5.27337193e-02\n",
      "  8.42273235e-01 5.71848273e-01 6.39941335e-01 2.56780803e-01\n",
      "  6.40344322e-01 8.32042098e-01 9.00215328e-01 7.72293210e-01\n",
      "  6.48332775e-01 7.90973306e-02 1.36815548e-01 5.80100238e-01\n",
      "  3.15897882e-01 5.18783569e-01 6.89334214e-01 6.80889785e-01\n",
      "  3.86283875e-01 1.87965035e-01 6.23934269e-02 7.09730148e-01\n",
      "  6.67503297e-01 1.15667701e-01 7.23704100e-01 5.93507528e-01\n",
      "  4.03985977e-01 4.95679379e-02 4.91213381e-01 7.85367608e-01\n",
      "  8.49081278e-02 4.22348022e-01 6.05522335e-01 7.62712955e-03\n",
      "  5.94029248e-01 7.51927674e-01 3.76603603e-01 7.82728195e-03\n",
      "  5.47520638e-01 1.83724880e-01 1.73055530e-01 9.56958711e-01\n",
      "  6.46012008e-01 2.95888722e-01 9.68019128e-01 7.60849595e-01\n",
      "  5.70346773e-01 3.15989256e-01 1.53137445e-01 7.78031528e-01\n",
      "  7.73728848e-01 7.97839940e-01 1.64800107e-01 9.86553788e-01]\n",
      " [3.86227369e-02 1.25496626e-01 4.04894352e-01 7.51665235e-01\n",
      "  3.86165142e-01 7.49224007e-01 4.76736426e-02 8.86657357e-01\n",
      "  5.10931015e-04 1.34858370e-01 1.63708687e-01 1.10132694e-02\n",
      "  9.44833755e-01 3.71987224e-02 5.72590709e-01 1.94214046e-01\n",
      "  7.90858984e-01 7.05545187e-01 4.34300840e-01 1.10120118e-01\n",
      "  9.52959597e-01 2.31981099e-01 8.91374290e-01 6.91585064e-01\n",
      "  2.15037286e-01 7.31276214e-01 2.83084691e-01 7.72114575e-01\n",
      "  6.79712892e-01 6.42469287e-01 4.41235185e-01 7.60931015e-01\n",
      "  6.47041798e-01 4.10179615e-01 4.49697495e-01 1.68069065e-01\n",
      "  3.71203780e-01 5.44021130e-02 3.92149091e-01 8.44066501e-01\n",
      "  1.74362957e-01 2.18847454e-01 7.62556851e-01 3.87865484e-01\n",
      "  3.10016572e-01 1.54477715e-01 9.74545956e-01 3.29063773e-01\n",
      "  9.98954177e-02 9.88650739e-01 6.52573109e-01 5.42201698e-01\n",
      "  3.32012177e-02 6.08380735e-01 3.85800004e-02 9.94435549e-02\n",
      "  3.70587945e-01 5.67439556e-01 8.12703311e-01 7.22803593e-01\n",
      "  9.89150524e-01 6.22431695e-01 9.45252538e-01 5.62336743e-01]\n",
      " [9.30389702e-01 9.57203805e-01 6.02860749e-01 5.00784934e-01\n",
      "  4.19523120e-02 5.40292442e-01 8.53241384e-01 3.95978332e-01\n",
      "  7.97448397e-01 7.13625014e-01 7.55493939e-01 9.67608213e-01\n",
      "  7.80746698e-01 2.16742098e-01 2.62462735e-01 5.59180379e-01\n",
      "  1.41640306e-02 2.88697600e-01 8.42095196e-01 6.16264105e-01\n",
      "  5.24106085e-01 1.60197616e-01 9.69386160e-01 1.89285874e-01\n",
      "  3.76922488e-01 1.87789798e-02 2.16145992e-01 3.71949017e-01\n",
      "  5.61459124e-01 9.83904958e-01 6.36232913e-01 5.37344813e-01\n",
      "  1.53801143e-01 9.16444778e-01 3.58307302e-01 6.41108692e-01\n",
      "  4.00000513e-01 4.75496173e-01 8.14941645e-01 7.71303475e-01\n",
      "  8.95029664e-01 7.30778575e-02 8.09870005e-01 1.18352950e-01\n",
      "  7.39745796e-01 2.82528222e-01 8.76462638e-01 5.99306703e-01\n",
      "  1.99416339e-01 4.29689288e-02 5.00367880e-02 3.65248084e-01\n",
      "  2.03307867e-02 1.29287183e-01 6.42733872e-01 6.23291433e-01\n",
      "  8.68488610e-01 5.89683831e-01 6.99635625e-01 4.46934700e-02\n",
      "  4.76578653e-01 6.05408013e-01 1.82951927e-01 7.66119361e-01]\n",
      " [4.80137467e-01 7.54385293e-01 6.78084791e-01 1.91964567e-01\n",
      "  1.24175847e-01 5.24498463e-01 7.98146248e-01 7.84461796e-01\n",
      "  6.64369404e-01 2.20489144e-01 5.78497350e-01 9.81009066e-01\n",
      "  4.32388604e-01 2.01206148e-01 6.94448829e-01 5.94334066e-01\n",
      "  2.99422979e-01 5.43331206e-01 1.04444087e-01 6.00723267e-01\n",
      "  9.33550417e-01 4.95535910e-01 4.57243145e-01 5.47450364e-01\n",
      "  1.39644146e-01 9.75488603e-01 5.60868442e-01 9.17564392e-01\n",
      "  9.21565115e-01 9.40795541e-01 2.73879409e-01 5.61406791e-01\n",
      "  9.22775328e-01 5.29540122e-01 1.21783614e-02 8.77326429e-01\n",
      "  8.65556777e-01 3.98889542e-01 1.65029168e-01 6.64422631e-01\n",
      "  5.92137575e-01 9.13898468e-01 7.94477582e-01 4.05710578e-01\n",
      "  8.62964809e-01 2.44289935e-01 7.65129685e-01 1.34245813e-01\n",
      "  7.05547035e-01 2.75976062e-02 6.71817720e-01 7.84642279e-01\n",
      "  1.54974043e-01 2.48255670e-01 1.66635334e-01 9.59616005e-01\n",
      "  2.50033379e-01 3.25504422e-01 3.14241409e-01 8.76096666e-01\n",
      "  6.79093242e-01 8.54979753e-02 2.48930693e-01 1.63851082e-01]]\n"
     ]
    }
   ],
   "source": [
    "input_feats = torch.rand([1,2,64]).to(\"cuda\")\n",
    "input_feats_np = input_feats.cpu().detach().numpy()\n",
    "input_feats_np = np.reshape(input_feats_np, (2,64))\n",
    "print(input_feats_np)\n",
    "dim = input_feats_np.shape\n",
    "\n",
    "flatten_inputs = input_feats_np.flatten()\n",
    "with open(os.path.join(\"ECAPAweights\", f\"ecapainput.bin\"), \"wb\") as f:\n",
    "            # Write the dimensions down\n",
    "            f.write(np.array(dim, dtype=np.int32).tobytes())\n",
    "            # Write the flatten bias down\n",
    "            f.write(flatten_inputs.tobytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ECAPA_TDNN(input_size = 2, channels=[8,8,8,8,16], lin_neurons=6, device = \"cuda\").to(\"cuda\")\n",
    "\n",
    "model_ = BlockSave(model.return_layers(), \"fullecapa\", \"ECAPAweights\")\n",
    "model_.save()\n",
    "\n",
    "initialblock = BlockSave(model.blocks[0].return_layers(), \"initialblock\", \"ECAPAweights\")\n",
    "initialblock.save()\n",
    "\n",
    "seres2_1 = BlockSave(model.blocks[1].return_layers(), \"seres2_1\", \"ECAPAweights\")\n",
    "seres2_1.save()\n",
    "seres2_2 = BlockSave(model.blocks[2].return_layers(), \"seres2_2\", \"ECAPAweights\")\n",
    "seres2_2.save()\n",
    "seres2_3 = BlockSave(model.blocks[3].return_layers(), \"seres2_3\", \"ECAPAweights\")\n",
    "seres2_3.save()\n",
    "\n",
    "mfa = BlockSave(model.mfa.return_layers(), \"mfa\", \"ECAPAweights\")\n",
    "mfa.save()\n",
    "\n",
    "asp = BlockSave(model.asp.return_layers(), \"asp\", \"ECAPAweights\")\n",
    "asp.save()\n",
    "\n",
    "# asp_bn = BlockSave(model.asp_bn.return_layers(), \"asp_bn\", \"ECAPAweights\")\n",
    "# asp_bn.save()\n",
    "\n",
    "fc = BlockSave([model.final[1]], \"fc\", \"ECAPAweights\")\n",
    "fc.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [8, 2, 5], expected input[1, 8, 68] to have 2 channels, but got 8 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\chiny\\OneDrive - Nanyang Technological University\\Internships\\AY24 DSO Summer\\LegacyCNN\\python_lib\\modules.py:577\u001b[0m, in \u001b[0;36mECAPA_TDNN.forward\u001b[1;34m(self, x, lengths)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 577\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\chiny\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'lengths'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_feats\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chiny\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\chiny\\OneDrive - Nanyang Technological University\\Internships\\AY24 DSO Summer\\LegacyCNN\\python_lib\\modules.py:579\u001b[0m, in \u001b[0;36mECAPA_TDNN.forward\u001b[1;34m(self, x, lengths)\u001b[0m\n\u001b[0;32m    577\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x, lengths\u001b[38;5;241m=\u001b[39mlengths)\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m--> 579\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    580\u001b[0m \u001b[38;5;66;03m# print(\"Output: \", x.shape)\u001b[39;00m\n\u001b[0;32m    581\u001b[0m xl\u001b[38;5;241m.\u001b[39mappend(x)\n",
      "File \u001b[1;32mc:\\Users\\chiny\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\chiny\\OneDrive - Nanyang Technological University\\Internships\\AY24 DSO Summer\\LegacyCNN\\python_lib\\modules.py:105\u001b[0m, in \u001b[0;36mTDNNBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Processes the input tensor x and returns an output tensor.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(x)\n\u001b[0;32m    107\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n",
      "File \u001b[1;32mc:\\Users\\chiny\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\chiny\\anaconda3\\envs\\pytorch\\lib\\site-packages\\speechbrain\\nnet\\CNN.py:445\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    441\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPadding must be \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcausal\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    442\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding\n\u001b[0;32m    443\u001b[0m     )\n\u001b[1;32m--> 445\u001b[0m wx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munsqueeze:\n\u001b[0;32m    448\u001b[0m     wx \u001b[38;5;241m=\u001b[39m wx\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\chiny\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\chiny\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py:313\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chiny\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py:309\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    307\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    308\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [8, 2, 5], expected input[1, 8, 68] to have 2 channels, but got 8 channels instead"
     ]
    }
   ],
   "source": [
    "model(input_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_feats.mean(dim=2, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def get_padding_elem(L_in: int, stride: int, kernel_size: int, dilation: int):\n",
    "    \"\"\"This function computes the number of elements to add for zero-padding.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    L_in : int\n",
    "    stride: int\n",
    "    kernel_size : int\n",
    "    dilation : int\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    padding : int\n",
    "        The size of the padding to be added\n",
    "    \"\"\"\n",
    "    if stride > 1:\n",
    "        padding = [math.floor(kernel_size / 2), math.floor(kernel_size / 2)]\n",
    "\n",
    "    else:\n",
    "        L_out = (\n",
    "            math.floor((L_in - dilation * (kernel_size - 1) - 1) / stride) + 1\n",
    "        )\n",
    "        padding = [\n",
    "            math.floor((L_in - L_out) / 2),\n",
    "            math.floor((L_in - L_out) / 2),\n",
    "        ]\n",
    "    return padding\n",
    "\n",
    "get_padding_elem(64,1,5,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
