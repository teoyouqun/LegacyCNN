{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chiny\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\chiny\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n",
      "SpeechBrain could not find any working torchaudio backend. Audio files may fail to load. Follow this link for instructions and troubleshooting: https://pytorch.org/audio/stable/index.html\n",
      "SpeechBrain could not find any working torchaudio backend. Audio files may fail to load. Follow this link for instructions and troubleshooting: https://pytorch.org/audio/stable/index.html\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "from python_lib.modules import *\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from python_lib.saveasfile import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.6582899e-02 7.1769017e-01 2.9701972e-01 7.9147923e-01 1.5550852e-04]\n",
      " [6.2197953e-01 3.4608144e-01 4.2699003e-01 7.5858295e-02 7.9285544e-01]]\n"
     ]
    }
   ],
   "source": [
    "input_feats = torch.rand([1,2,64]).to(\"cuda\")\n",
    "input_feats_np = input_feats.cpu().detach().numpy()\n",
    "input_feats_np = np.reshape(input_feats_np, (2,64))\n",
    "print(input_feats_np)\n",
    "dim = input_feats_np.shape\n",
    "\n",
    "flatten_inputs = input_feats_np.flatten()\n",
    "with open(os.path.join(\"ECAPAweights\", f\"ecapainput.bin\"), \"wb\") as f:\n",
    "            # Write the dimensions down\n",
    "            f.write(np.array(dim, dtype=np.int32).tobytes())\n",
    "            # Write the flatten bias down\n",
    "            f.write(flatten_inputs.tobytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1417, 0.2664, 0.1749, 0.2868, 0.1300],\n",
       "         [0.2300, 0.1746, 0.1893, 0.1332, 0.2729]]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "F.softmax(input_feats, dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ECAPA_TDNN(input_size = 2, channels=[8,8,8,8,16], lin_neurons=6, device = \"cuda\").to(\"cuda\")\n",
    "\n",
    "model_ = BlockSave(model.return_layers(), \"fullecapa\", \"ECAPAweights\")\n",
    "model_.save()\n",
    "\n",
    "initialblock = BlockSave(model.blocks[0].return_layers(), \"initialblock\", \"ECAPAweights\")\n",
    "initialblock.save()\n",
    "\n",
    "seres2_1 = BlockSave(model.blocks[1].return_layers(), \"seres2_1\", \"ECAPAweights\")\n",
    "seres2_1.save()\n",
    "seres2_2 = BlockSave(model.blocks[2].return_layers(), \"seres2_2\", \"ECAPAweights\")\n",
    "seres2_2.save()\n",
    "seres2_3 = BlockSave(model.blocks[3].return_layers(), \"seres2_3\", \"ECAPAweights\")\n",
    "seres2_3.save()\n",
    "\n",
    "mfa = BlockSave(model.mfa.return_layers(), \"mfa\", \"ECAPAweights\")\n",
    "mfa.save()\n",
    "\n",
    "asp = BlockSave(model.asp.return_layers(), \"asp\", \"ECAPAweights\")\n",
    "asp.save()\n",
    "\n",
    "# asp_bn = BlockSave(model.asp_bn.return_layers(), \"asp_bn\", \"ECAPAweights\")\n",
    "# asp_bn.save()\n",
    "\n",
    "fc = BlockSave([model.final[1]], \"fc\", \"ECAPAweights\")\n",
    "fc.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2879,  0.2103, -0.1322,  ...,  0.2579,  0.1210,  0.2716],\n",
      "         [ 0.3241,  0.3321,  0.1543,  ...,  0.4876,  0.4087,  0.1353],\n",
      "         [-0.1158,  0.0232, -0.1112,  ...,  0.2624, -0.3457,  0.3734],\n",
      "         ...,\n",
      "         [-0.2016, -0.2271, -0.0020,  ..., -0.2858, -0.2350,  0.3585],\n",
      "         [-0.1578, -0.2103, -0.0109,  ...,  0.0560, -0.5549, -0.1462],\n",
      "         [-0.5445,  0.2431,  0.4866,  ..., -0.0935,  0.0419, -0.0091]]],\n",
      "       device='cuda:0', grad_fn=<MaskedFillBackward0>)\n",
      "tensor([[[0.0208, 0.0192, 0.0136,  ..., 0.0202, 0.0176, 0.0204],\n",
      "         [0.0195, 0.0197, 0.0165,  ..., 0.0230, 0.0212, 0.0161],\n",
      "         [0.0126, 0.0145, 0.0127,  ..., 0.0184, 0.0100, 0.0206],\n",
      "         ...,\n",
      "         [0.0129, 0.0126, 0.0157,  ..., 0.0118, 0.0125, 0.0226],\n",
      "         [0.0140, 0.0133, 0.0163,  ..., 0.0174, 0.0094, 0.0142],\n",
      "         [0.0091, 0.0200, 0.0256,  ..., 0.0143, 0.0164, 0.0156]]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4871],\n",
       "         [0.5331],\n",
       "         [0.4985],\n",
       "         [0.4988],\n",
       "         [0.4724],\n",
       "         [0.6055],\n",
       "         [0.4554],\n",
       "         [0.4931],\n",
       "         [0.5001],\n",
       "         [0.3983],\n",
       "         [0.4776],\n",
       "         [0.6082],\n",
       "         [0.5276],\n",
       "         [0.5281],\n",
       "         [0.4740],\n",
       "         [0.4087],\n",
       "         [0.2977],\n",
       "         [0.3070],\n",
       "         [0.2811],\n",
       "         [0.3147],\n",
       "         [0.2863],\n",
       "         [0.2687],\n",
       "         [0.2477],\n",
       "         [0.2496],\n",
       "         [0.2593],\n",
       "         [0.2683],\n",
       "         [0.2836],\n",
       "         [0.2639],\n",
       "         [0.3006],\n",
       "         [0.2704],\n",
       "         [0.2908],\n",
       "         [0.2660]]], device='cuda:0', grad_fn=<UnsqueezeBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.asp(input_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_feats.mean(dim=2, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def get_padding_elem(L_in: int, stride: int, kernel_size: int, dilation: int):\n",
    "    \"\"\"This function computes the number of elements to add for zero-padding.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    L_in : int\n",
    "    stride: int\n",
    "    kernel_size : int\n",
    "    dilation : int\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    padding : int\n",
    "        The size of the padding to be added\n",
    "    \"\"\"\n",
    "    if stride > 1:\n",
    "        padding = [math.floor(kernel_size / 2), math.floor(kernel_size / 2)]\n",
    "\n",
    "    else:\n",
    "        L_out = (\n",
    "            math.floor((L_in - dilation * (kernel_size - 1) - 1) / stride) + 1\n",
    "        )\n",
    "        padding = [\n",
    "            math.floor((L_in - L_out) / 2),\n",
    "            math.floor((L_in - L_out) / 2),\n",
    "        ]\n",
    "    return padding\n",
    "\n",
    "get_padding_elem(64,1,5,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
