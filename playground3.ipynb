{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "from python_lib.modules import *\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from python_lib.saveasfile import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.75369895 0.47365534 0.04631549 0.57971156 0.49222088 0.6482848\n",
      "  0.6341973  0.40538418 0.06989694 0.5507403  0.60797036 0.5018034\n",
      "  0.78907204 0.1264118  0.03590226 0.41581845 0.31956482 0.31640375\n",
      "  0.07271683 0.8352609  0.5753878  0.31308526 0.48919457 0.22090179\n",
      "  0.04806745 0.25747478 0.6491883  0.6253344  0.72392887 0.6715279\n",
      "  0.4152034  0.8586629  0.41310942 0.18586099 0.05589753 0.40654135\n",
      "  0.7927932  0.14876276 0.6241265  0.20615232 0.60735464 0.7453405\n",
      "  0.49164325 0.44260287 0.09521008 0.8434055  0.06873977 0.41850448\n",
      "  0.11352092 0.18141454 0.16695625 0.8584323  0.7359674  0.3960212\n",
      "  0.06126249 0.85519814 0.5904064  0.52389294 0.4231736  0.60239846\n",
      "  0.51771396 0.6465695  0.5198399  0.37611926]\n",
      " [0.04829127 0.9093083  0.7491111  0.00501996 0.78101456 0.22385031\n",
      "  0.37785482 0.01673681 0.7843546  0.41122788 0.7546224  0.8538427\n",
      "  0.08612823 0.34328443 0.63188034 0.538451   0.3305779  0.88262796\n",
      "  0.09293908 0.5321195  0.14994687 0.8919201  0.06027186 0.14615977\n",
      "  0.98546565 0.22726464 0.17934287 0.54202944 0.49173307 0.2134906\n",
      "  0.87878966 0.05395764 0.62868786 0.47522002 0.63892376 0.5280564\n",
      "  0.6093875  0.91817075 0.5340698  0.4831047  0.51635855 0.7915799\n",
      "  0.24247271 0.48672724 0.4178055  0.29334873 0.58820903 0.7502684\n",
      "  0.47499663 0.18636751 0.27052903 0.5525803  0.5632831  0.4678343\n",
      "  0.7818056  0.69068563 0.31364423 0.70934147 0.7336352  0.6661521\n",
      "  0.31162423 0.01571089 0.78049827 0.52153295]]\n"
     ]
    }
   ],
   "source": [
    "input_feats = torch.rand([1,2,64]).to(\"cuda\")\n",
    "input_feats_np = input_feats.cpu().detach().numpy()\n",
    "input_feats_np = np.reshape(input_feats_np, (2,64))\n",
    "print(input_feats_np)\n",
    "dim = input_feats_np.shape\n",
    "\n",
    "flatten_inputs = input_feats_np.flatten()\n",
    "with open(os.path.join(\"ECAPAweights\", f\"ecapainput.bin\"), \"wb\") as f:\n",
    "            # Write the dimensions down\n",
    "            f.write(np.array(dim, dtype=np.int32).tobytes())\n",
    "            # Write the flatten bias down\n",
    "            f.write(flatten_inputs.tobytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0206, 0.0156, 0.0102, 0.0173, 0.0159, 0.0185, 0.0183, 0.0145,\n",
       "          0.0104, 0.0168, 0.0178, 0.0160, 0.0213, 0.0110, 0.0100, 0.0147,\n",
       "          0.0133, 0.0133, 0.0104, 0.0223, 0.0172, 0.0133, 0.0158, 0.0121,\n",
       "          0.0102, 0.0125, 0.0186, 0.0181, 0.0200, 0.0190, 0.0147, 0.0229,\n",
       "          0.0147, 0.0117, 0.0103, 0.0146, 0.0214, 0.0112, 0.0181, 0.0119,\n",
       "          0.0178, 0.0204, 0.0158, 0.0151, 0.0107, 0.0225, 0.0104, 0.0147,\n",
       "          0.0109, 0.0116, 0.0115, 0.0229, 0.0202, 0.0144, 0.0103, 0.0228,\n",
       "          0.0175, 0.0164, 0.0148, 0.0177, 0.0163, 0.0185, 0.0163, 0.0141],\n",
       "         [0.0097, 0.0230, 0.0196, 0.0093, 0.0203, 0.0116, 0.0135, 0.0094,\n",
       "          0.0203, 0.0140, 0.0197, 0.0218, 0.0101, 0.0131, 0.0174, 0.0159,\n",
       "          0.0129, 0.0224, 0.0102, 0.0158, 0.0108, 0.0226, 0.0099, 0.0107,\n",
       "          0.0248, 0.0116, 0.0111, 0.0159, 0.0152, 0.0115, 0.0223, 0.0098,\n",
       "          0.0174, 0.0149, 0.0176, 0.0157, 0.0171, 0.0232, 0.0158, 0.0150,\n",
       "          0.0155, 0.0205, 0.0118, 0.0151, 0.0141, 0.0124, 0.0167, 0.0196,\n",
       "          0.0149, 0.0112, 0.0122, 0.0161, 0.0163, 0.0148, 0.0203, 0.0185,\n",
       "          0.0127, 0.0189, 0.0193, 0.0181, 0.0127, 0.0094, 0.0202, 0.0156]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "F.softmax(input_feats, dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ECAPA_TDNN(input_size = 2, channels=[8,8,8,8,16], lin_neurons=6, device = \"cuda\").to(\"cuda\")\n",
    "\n",
    "model_ = BlockSave(model.return_layers(), \"fullecapa\", \"ECAPAweights\")\n",
    "model_.save()\n",
    "\n",
    "initialblock = BlockSave(model.blocks[0].return_layers(), \"initialblock\", \"ECAPAweights\")\n",
    "initialblock.save()\n",
    "\n",
    "seres2_1 = BlockSave(model.blocks[1].return_layers(), \"seres2_1\", \"ECAPAweights\")\n",
    "seres2_1.save()\n",
    "seres2_2 = BlockSave(model.blocks[2].return_layers(), \"seres2_2\", \"ECAPAweights\")\n",
    "seres2_2.save()\n",
    "seres2_3 = BlockSave(model.blocks[3].return_layers(), \"seres2_3\", \"ECAPAweights\")\n",
    "seres2_3.save()\n",
    "\n",
    "mfa = BlockSave(model.mfa.return_layers(), \"mfa\", \"ECAPAweights\")\n",
    "mfa.save()\n",
    "\n",
    "asp = BlockSave(model.asp.return_layers(), \"asp\", \"ECAPAweights\")\n",
    "asp.save()\n",
    "\n",
    "# asp_bn = BlockSave(model.asp_bn.return_layers(), \"asp_bn\", \"ECAPAweights\")\n",
    "# asp_bn.save()\n",
    "\n",
    "fc = BlockSave([model.final[1]], \"fc\", \"ECAPAweights\")\n",
    "fc.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.4901e-08],\n",
      "         [ 7.4506e-09],\n",
      "         [ 0.0000e+00],\n",
      "         [ 0.0000e+00],\n",
      "         [-2.2352e-08],\n",
      "         [-7.8231e-08],\n",
      "         [-2.9802e-08],\n",
      "         [ 3.7253e-09]]], device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor([[[0.0000],\n",
      "         [0.3120],\n",
      "         [0.1469],\n",
      "         [0.0000],\n",
      "         [0.2502],\n",
      "         [0.0677],\n",
      "         [0.1094],\n",
      "         [0.3239],\n",
      "         [0.0000],\n",
      "         [0.2753],\n",
      "         [0.0000],\n",
      "         [0.3054],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0483],\n",
      "         [0.0000],\n",
      "         [0.3145],\n",
      "         [0.2334],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.1488],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.1784],\n",
      "         [0.1203],\n",
      "         [0.0000],\n",
      "         [0.3197],\n",
      "         [0.2059],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.2272],\n",
      "         [0.0000],\n",
      "         [0.2815],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.2816],\n",
      "         [0.3235],\n",
      "         [0.0000],\n",
      "         [0.3394],\n",
      "         [0.0870],\n",
      "         [0.2837],\n",
      "         [0.1009],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0506],\n",
      "         [0.0000],\n",
      "         [0.0079],\n",
      "         [0.2589],\n",
      "         [0.0000],\n",
      "         [0.0116],\n",
      "         [0.0000],\n",
      "         [0.0090],\n",
      "         [0.0000],\n",
      "         [0.0201],\n",
      "         [0.1541],\n",
      "         [0.0000],\n",
      "         [0.2381],\n",
      "         [0.2945],\n",
      "         [0.2198],\n",
      "         [0.0588],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0458],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.2859],\n",
      "         [0.2638],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.1083],\n",
      "         [0.1421],\n",
      "         [0.0000],\n",
      "         [0.1785],\n",
      "         [0.2662],\n",
      "         [0.0000],\n",
      "         [0.0173],\n",
      "         [0.1075],\n",
      "         [0.0000],\n",
      "         [0.1389],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0538],\n",
      "         [0.3242],\n",
      "         [0.0000],\n",
      "         [0.1205],\n",
      "         [0.1533],\n",
      "         [0.3392],\n",
      "         [0.0000],\n",
      "         [0.2340],\n",
      "         [0.0080],\n",
      "         [0.3153],\n",
      "         [0.2378],\n",
      "         [0.0269],\n",
      "         [0.0000],\n",
      "         [0.1644],\n",
      "         [0.2680],\n",
      "         [0.0000],\n",
      "         [0.0307],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.1379],\n",
      "         [0.0000],\n",
      "         [0.0730],\n",
      "         [0.2429],\n",
      "         [0.0000],\n",
      "         [0.0116],\n",
      "         [0.2597],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.1272],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0724],\n",
      "         [0.2312],\n",
      "         [0.0000],\n",
      "         [0.1262]]], device='cuda:0', grad_fn=<ReluBackward0>)\n",
      "tensor([[[0.5174],\n",
      "         [0.5186],\n",
      "         [0.4967],\n",
      "         [0.4916],\n",
      "         [0.5235],\n",
      "         [0.4894],\n",
      "         [0.5256],\n",
      "         [0.4666]]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "tensor([[[-1.4901e-08],\n",
      "         [-2.6077e-08],\n",
      "         [-5.2154e-08],\n",
      "         [ 2.6077e-08],\n",
      "         [ 2.2352e-08],\n",
      "         [ 4.4703e-08],\n",
      "         [-7.4506e-09],\n",
      "         [-4.4703e-08]]], device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor([[[0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0412],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0186],\n",
      "         [0.1538],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.2544],\n",
      "         [0.0965],\n",
      "         [0.0000],\n",
      "         [0.2583],\n",
      "         [0.1511],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0107],\n",
      "         [0.2113],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.1978],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0770],\n",
      "         [0.2484],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.2631],\n",
      "         [0.2538],\n",
      "         [0.0000],\n",
      "         [0.2281],\n",
      "         [0.1143],\n",
      "         [0.1196],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0885],\n",
      "         [0.2399],\n",
      "         [0.0512],\n",
      "         [0.0055],\n",
      "         [0.0000],\n",
      "         [0.0288],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0371],\n",
      "         [0.0000],\n",
      "         [0.1843],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.1857],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.2976],\n",
      "         [0.1064],\n",
      "         [0.1271],\n",
      "         [0.1098],\n",
      "         [0.0000],\n",
      "         [0.2910],\n",
      "         [0.3048],\n",
      "         [0.0000],\n",
      "         [0.0321],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.2692],\n",
      "         [0.3260],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.1017],\n",
      "         [0.1236],\n",
      "         [0.2988],\n",
      "         [0.2585],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.3206],\n",
      "         [0.0462],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0723],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.2463],\n",
      "         [0.3513],\n",
      "         [0.2639],\n",
      "         [0.0000],\n",
      "         [0.0492],\n",
      "         [0.0000],\n",
      "         [0.2153],\n",
      "         [0.3503],\n",
      "         [0.2437],\n",
      "         [0.0921],\n",
      "         [0.0947],\n",
      "         [0.2761],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.1468],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.3328],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.1758],\n",
      "         [0.1144],\n",
      "         [0.0000],\n",
      "         [0.0000]]], device='cuda:0', grad_fn=<ReluBackward0>)\n",
      "tensor([[[0.4984],\n",
      "         [0.5179],\n",
      "         [0.4925],\n",
      "         [0.5125],\n",
      "         [0.5109],\n",
      "         [0.5108],\n",
      "         [0.4883],\n",
      "         [0.4939]]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "tensor([[[ 9.6858e-08],\n",
      "         [ 7.4506e-09],\n",
      "         [ 1.8626e-08],\n",
      "         [ 5.5879e-08],\n",
      "         [ 0.0000e+00],\n",
      "         [-1.8626e-08],\n",
      "         [ 4.4703e-08],\n",
      "         [-2.2352e-08]]], device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor([[[0.1377],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.2726],\n",
      "         [0.0000],\n",
      "         [0.3012],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.3197],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.2848],\n",
      "         [0.1816],\n",
      "         [0.0000],\n",
      "         [0.2502],\n",
      "         [0.0855],\n",
      "         [0.0000],\n",
      "         [0.3430],\n",
      "         [0.0000],\n",
      "         [0.3429],\n",
      "         [0.1705],\n",
      "         [0.2976],\n",
      "         [0.0000],\n",
      "         [0.0287],\n",
      "         [0.0000],\n",
      "         [0.1618],\n",
      "         [0.3064],\n",
      "         [0.0000],\n",
      "         [0.2562],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0266],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.2311],\n",
      "         [0.3020],\n",
      "         [0.2332],\n",
      "         [0.3259],\n",
      "         [0.2702],\n",
      "         [0.2471],\n",
      "         [0.1671],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0353],\n",
      "         [0.0963],\n",
      "         [0.1337],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.2749],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0927],\n",
      "         [0.1590],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.1102],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0133],\n",
      "         [0.0000],\n",
      "         [0.1204],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0962],\n",
      "         [0.0000],\n",
      "         [0.1333],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0559],\n",
      "         [0.0000],\n",
      "         [0.2917],\n",
      "         [0.0000],\n",
      "         [0.2626],\n",
      "         [0.2986],\n",
      "         [0.2533],\n",
      "         [0.3392],\n",
      "         [0.1292],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.1016],\n",
      "         [0.0000],\n",
      "         [0.3346],\n",
      "         [0.0235],\n",
      "         [0.0000],\n",
      "         [0.2090],\n",
      "         [0.0082],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.2678],\n",
      "         [0.0000],\n",
      "         [0.2177],\n",
      "         [0.1721],\n",
      "         [0.0584],\n",
      "         [0.2337],\n",
      "         [0.0000],\n",
      "         [0.1911],\n",
      "         [0.1264],\n",
      "         [0.3503],\n",
      "         [0.0000],\n",
      "         [0.2966],\n",
      "         [0.0132],\n",
      "         [0.3243],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.2835],\n",
      "         [0.2424],\n",
      "         [0.1304],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.2705],\n",
      "         [0.0000],\n",
      "         [0.1151]]], device='cuda:0', grad_fn=<ReluBackward0>)\n",
      "tensor([[[0.4694],\n",
      "         [0.4301],\n",
      "         [0.4934],\n",
      "         [0.5134],\n",
      "         [0.5680],\n",
      "         [0.4938],\n",
      "         [0.5341],\n",
      "         [0.4796]]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "MLA Input:  tensor([[[-0.2778,  1.5095, -0.2778,  ..., -0.2778, -0.2778,  0.2025],\n",
      "         [-1.0085,  3.5742, -1.0801,  ..., -0.1407,  0.7175,  0.2610],\n",
      "         [-0.7304, -1.0649,  0.4388,  ..., -1.0239, -0.2126,  3.1270],\n",
      "         ...,\n",
      "         [-0.6083,  1.0798, -1.1814,  ..., -0.9996, -0.1947,  0.3735],\n",
      "         [-1.1420, -1.7953,  3.6062,  ..., -0.7896, -0.4202,  1.4747],\n",
      "         [ 1.0384,  2.1581,  0.1683,  ...,  3.2143, -0.4657, -0.3322]]],\n",
      "       device='cuda:0', grad_fn=<CatBackward0>)\n",
      "MLA Output:  tensor([[[-0.7687,  0.4931,  3.7611,  ...,  0.0180, -0.6159,  0.7871],\n",
      "         [ 1.2141, -0.6838, -0.6838,  ..., -0.6838, -0.6838, -0.6838],\n",
      "         [-0.7126,  3.9930,  0.7345,  ...,  3.1330,  0.2925, -0.7126],\n",
      "         ...,\n",
      "         [-0.5509,  1.7032, -0.5509,  ...,  2.5711,  1.2805, -0.5509],\n",
      "         [ 1.0377,  0.3878, -0.7595,  ..., -0.7595, -0.7595, -0.7595],\n",
      "         [ 0.8668, -0.7728, -0.7728,  ..., -0.5963, -0.7728, -0.7728]]],\n",
      "       device='cuda:0', grad_fn=<CudnnBatchNormBackward0>)\n",
      "ASP Input:  tensor([[[-0.7687,  0.4931,  3.7611,  ...,  0.0180, -0.6159,  0.7871],\n",
      "         [ 1.2141, -0.6838, -0.6838,  ..., -0.6838, -0.6838, -0.6838],\n",
      "         [-0.7126,  3.9930,  0.7345,  ...,  3.1330,  0.2925, -0.7126],\n",
      "         ...,\n",
      "         [-0.5509,  1.7032, -0.5509,  ...,  2.5711,  1.2805, -0.5509],\n",
      "         [ 1.0377,  0.3878, -0.7595,  ..., -0.7595, -0.7595, -0.7595],\n",
      "         [ 0.8668, -0.7728, -0.7728,  ..., -0.5963, -0.7728, -0.7728]]],\n",
      "       device='cuda:0', grad_fn=<CudnnBatchNormBackward0>)\n",
      "tensor([[[-0.2152, -0.2454, -0.4137,  ...,  0.2880,  0.3324, -0.0474],\n",
      "         [-0.0447, -0.0530, -0.0372,  ..., -0.0359,  0.4987, -0.0335],\n",
      "         [ 0.2112, -0.2286,  0.4689,  ..., -0.6964, -0.1218, -0.3326],\n",
      "         ...,\n",
      "         [-0.3212,  0.7704,  0.5188,  ...,  0.3870, -0.2661, -0.0219],\n",
      "         [-0.2905, -0.3761, -0.3204,  ...,  0.3035,  0.2167,  0.0459],\n",
      "         [-0.3970, -0.0333,  0.1168,  ...,  0.1868,  0.0693,  0.4034]]],\n",
      "       device='cuda:0', grad_fn=<MaskedFillBackward0>)\n",
      "tensor([[[0.0122, 0.0119, 0.0100,  ..., 0.0202, 0.0212, 0.0145],\n",
      "         [0.0136, 0.0135, 0.0137,  ..., 0.0137, 0.0234, 0.0137],\n",
      "         [0.0180, 0.0116, 0.0233,  ..., 0.0073, 0.0129, 0.0105],\n",
      "         ...,\n",
      "         [0.0112, 0.0334, 0.0260,  ..., 0.0228, 0.0118, 0.0151],\n",
      "         [0.0124, 0.0114, 0.0120,  ..., 0.0225, 0.0206, 0.0174],\n",
      "         [0.0101, 0.0145, 0.0169,  ..., 0.0181, 0.0161, 0.0225]]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "ASP Output:  tensor([[[-1.1754e-01],\n",
      "         [ 2.2556e-02],\n",
      "         [-4.2727e-02],\n",
      "         [ 2.2286e-01],\n",
      "         [-3.5299e-02],\n",
      "         [-3.4951e-02],\n",
      "         [ 3.3632e-03],\n",
      "         [ 1.2516e-01],\n",
      "         [-7.5258e-02],\n",
      "         [ 1.3666e-01],\n",
      "         [-3.1465e-02],\n",
      "         [ 4.3403e-02],\n",
      "         [ 6.5884e-04],\n",
      "         [ 2.1046e-01],\n",
      "         [-1.2092e-01],\n",
      "         [-1.3896e-01],\n",
      "         [ 8.9880e-01],\n",
      "         [ 9.9087e-01],\n",
      "         [ 9.1211e-01],\n",
      "         [ 1.1683e+00],\n",
      "         [ 9.9497e-01],\n",
      "         [ 9.3320e-01],\n",
      "         [ 9.8103e-01],\n",
      "         [ 1.0654e+00],\n",
      "         [ 9.3233e-01],\n",
      "         [ 1.0850e+00],\n",
      "         [ 9.4182e-01],\n",
      "         [ 1.0212e+00],\n",
      "         [ 9.9555e-01],\n",
      "         [ 1.1975e+00],\n",
      "         [ 9.4610e-01],\n",
      "         [ 9.2956e-01]]], device='cuda:0', grad_fn=<UnsqueezeBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0559, 0.1844, 0.1900, 0.2514, 0.2227, 0.0956]], device='cuda:0',\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_feats.mean(dim=2, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def get_padding_elem(L_in: int, stride: int, kernel_size: int, dilation: int):\n",
    "    \"\"\"This function computes the number of elements to add for zero-padding.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    L_in : int\n",
    "    stride: int\n",
    "    kernel_size : int\n",
    "    dilation : int\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    padding : int\n",
    "        The size of the padding to be added\n",
    "    \"\"\"\n",
    "    if stride > 1:\n",
    "        padding = [math.floor(kernel_size / 2), math.floor(kernel_size / 2)]\n",
    "\n",
    "    else:\n",
    "        L_out = (\n",
    "            math.floor((L_in - dilation * (kernel_size - 1) - 1) / stride) + 1\n",
    "        )\n",
    "        padding = [\n",
    "            math.floor((L_in - L_out) / 2),\n",
    "            math.floor((L_in - L_out) / 2),\n",
    "        ]\n",
    "    return padding\n",
    "\n",
    "get_padding_elem(64,1,5,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
