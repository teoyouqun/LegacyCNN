{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary libraries and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_lib.modules import *\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from python_lib.saveasfile import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model initialisation \n",
    "\n",
    "Use BlockSave to save all the weights into a `.bin` file\n",
    "\n",
    "Be sure to `load_state_dict` from a `.pt`/`.pth` file before BlockSave\n",
    "\n",
    "It's omitted here as I don't have the weights on hand, so I just use a random initialised weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<python_lib.saveasfile.BlockSave at 0x1e48a0c9280>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ECAPA_TDNN(input_size = 2, channels=[8,8,8,8,16], lin_neurons=6, device = \"cuda\").to(\"cuda\")\n",
    "\n",
    "BlockSave(model.return_layers(), \"fullecapa\", \"ECAPAweights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Input Feats for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.13175988 0.83884805 0.18142343 ... 0.03749532 0.9003383  0.71930987]\n",
      "  [0.07325369 0.23795128 0.22634566 ... 0.49048    0.2460568  0.5732361 ]]\n",
      "\n",
      " [[0.9554553  0.72399485 0.04407668 ... 0.4900689  0.16904444 0.03300488]\n",
      "  [0.676814   0.36807913 0.9845918  ... 0.10286719 0.968257   0.07335418]]\n",
      "\n",
      " [[0.03088188 0.36878616 0.0039925  ... 0.08633441 0.27036703 0.0772416 ]\n",
      "  [0.78553414 0.67571807 0.88156545 ... 0.665303   0.21483499 0.41274375]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.731148   0.36628282 0.24018592 ... 0.47873044 0.08777386 0.61594766]\n",
      "  [0.9290078  0.33120102 0.05099201 ... 0.6845378  0.9484555  0.8847231 ]]\n",
      "\n",
      " [[0.8790486  0.74943656 0.5798448  ... 0.57165754 0.8633431  0.40699255]\n",
      "  [0.80669284 0.44744015 0.24166703 ... 0.2780884  0.9402747  0.19634408]]\n",
      "\n",
      " [[0.06906515 0.6387082  0.38524663 ... 0.47731018 0.9629268  0.57335514]\n",
      "  [0.52897197 0.09806246 0.5458225  ... 0.12157243 0.49805576 0.940619  ]]]\n",
      "(100, 2, 64)\n"
     ]
    }
   ],
   "source": [
    "input_feats = torch.rand([100,2,64]).to(\"cuda\")\n",
    "input_feats_np = input_feats.cpu().detach().numpy()\n",
    "input_feats_np = np.reshape(input_feats_np, (100,2,64))\n",
    "print(input_feats_np)\n",
    "dim = input_feats_np.shape\n",
    "print(dim)\n",
    "\n",
    "flatten_inputs = input_feats_np.flatten()\n",
    "with open(os.path.join(\"ECAPAweights\", f\"ecapainput_2x64_100.bin\"), \"wb\") as f:\n",
    "            # Write the dimensions down\n",
    "            f.write(np.array(dim, dtype=np.int32).tobytes())\n",
    "            # Write the flatten bias down\n",
    "            f.write(flatten_inputs.tobytes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the model using eval/train mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1886, 0.1417, 0.1833, 0.1605, 0.1608, 0.1652],\n",
       "        [0.1889, 0.1416, 0.1829, 0.1611, 0.1604, 0.1651],\n",
       "        [0.1888, 0.1418, 0.1827, 0.1610, 0.1604, 0.1653],\n",
       "        [0.1890, 0.1416, 0.1833, 0.1606, 0.1603, 0.1653],\n",
       "        [0.1888, 0.1414, 0.1832, 0.1605, 0.1606, 0.1655],\n",
       "        [0.1892, 0.1416, 0.1832, 0.1609, 0.1602, 0.1650],\n",
       "        [0.1894, 0.1413, 0.1829, 0.1608, 0.1601, 0.1654],\n",
       "        [0.1892, 0.1415, 0.1833, 0.1610, 0.1600, 0.1650],\n",
       "        [0.1891, 0.1418, 0.1832, 0.1611, 0.1598, 0.1650],\n",
       "        [0.1893, 0.1413, 0.1828, 0.1608, 0.1604, 0.1654],\n",
       "        [0.1890, 0.1415, 0.1832, 0.1607, 0.1603, 0.1652],\n",
       "        [0.1894, 0.1415, 0.1832, 0.1610, 0.1598, 0.1651],\n",
       "        [0.1888, 0.1416, 0.1830, 0.1608, 0.1605, 0.1653],\n",
       "        [0.1893, 0.1413, 0.1829, 0.1609, 0.1602, 0.1653],\n",
       "        [0.1895, 0.1411, 0.1826, 0.1611, 0.1603, 0.1654],\n",
       "        [0.1889, 0.1416, 0.1834, 0.1606, 0.1602, 0.1652],\n",
       "        [0.1893, 0.1415, 0.1833, 0.1609, 0.1599, 0.1652],\n",
       "        [0.1891, 0.1414, 0.1832, 0.1608, 0.1603, 0.1653],\n",
       "        [0.1890, 0.1416, 0.1831, 0.1608, 0.1603, 0.1653],\n",
       "        [0.1889, 0.1416, 0.1832, 0.1611, 0.1601, 0.1651],\n",
       "        [0.1891, 0.1413, 0.1830, 0.1607, 0.1604, 0.1655],\n",
       "        [0.1894, 0.1414, 0.1828, 0.1612, 0.1600, 0.1652],\n",
       "        [0.1889, 0.1413, 0.1828, 0.1608, 0.1606, 0.1655],\n",
       "        [0.1890, 0.1416, 0.1830, 0.1610, 0.1603, 0.1651],\n",
       "        [0.1893, 0.1411, 0.1828, 0.1608, 0.1605, 0.1654],\n",
       "        [0.1889, 0.1415, 0.1832, 0.1609, 0.1602, 0.1653],\n",
       "        [0.1890, 0.1416, 0.1832, 0.1609, 0.1603, 0.1651],\n",
       "        [0.1892, 0.1414, 0.1829, 0.1609, 0.1604, 0.1654],\n",
       "        [0.1896, 0.1412, 0.1828, 0.1610, 0.1601, 0.1654],\n",
       "        [0.1890, 0.1414, 0.1828, 0.1607, 0.1605, 0.1656],\n",
       "        [0.1888, 0.1419, 0.1834, 0.1606, 0.1601, 0.1653],\n",
       "        [0.1891, 0.1412, 0.1828, 0.1609, 0.1605, 0.1655],\n",
       "        [0.1890, 0.1414, 0.1830, 0.1607, 0.1606, 0.1654],\n",
       "        [0.1892, 0.1412, 0.1830, 0.1611, 0.1604, 0.1651],\n",
       "        [0.1892, 0.1414, 0.1824, 0.1612, 0.1604, 0.1654],\n",
       "        [0.1897, 0.1411, 0.1829, 0.1607, 0.1601, 0.1656],\n",
       "        [0.1891, 0.1416, 0.1831, 0.1609, 0.1601, 0.1652],\n",
       "        [0.1889, 0.1415, 0.1833, 0.1608, 0.1603, 0.1652],\n",
       "        [0.1892, 0.1418, 0.1832, 0.1608, 0.1598, 0.1651],\n",
       "        [0.1892, 0.1415, 0.1832, 0.1605, 0.1602, 0.1654],\n",
       "        [0.1895, 0.1414, 0.1828, 0.1612, 0.1599, 0.1652],\n",
       "        [0.1893, 0.1413, 0.1830, 0.1609, 0.1600, 0.1654],\n",
       "        [0.1891, 0.1415, 0.1826, 0.1609, 0.1604, 0.1655],\n",
       "        [0.1889, 0.1415, 0.1833, 0.1608, 0.1602, 0.1653],\n",
       "        [0.1894, 0.1414, 0.1831, 0.1609, 0.1600, 0.1652],\n",
       "        [0.1892, 0.1413, 0.1830, 0.1609, 0.1602, 0.1654],\n",
       "        [0.1890, 0.1414, 0.1832, 0.1607, 0.1605, 0.1653],\n",
       "        [0.1888, 0.1416, 0.1833, 0.1606, 0.1605, 0.1653],\n",
       "        [0.1894, 0.1412, 0.1832, 0.1610, 0.1601, 0.1651],\n",
       "        [0.1890, 0.1414, 0.1828, 0.1609, 0.1604, 0.1655],\n",
       "        [0.1888, 0.1416, 0.1831, 0.1604, 0.1607, 0.1654],\n",
       "        [0.1893, 0.1413, 0.1832, 0.1610, 0.1602, 0.1650],\n",
       "        [0.1892, 0.1414, 0.1828, 0.1606, 0.1605, 0.1655],\n",
       "        [0.1894, 0.1411, 0.1830, 0.1609, 0.1603, 0.1653],\n",
       "        [0.1891, 0.1413, 0.1832, 0.1607, 0.1604, 0.1653],\n",
       "        [0.1891, 0.1414, 0.1829, 0.1607, 0.1604, 0.1656],\n",
       "        [0.1890, 0.1415, 0.1828, 0.1605, 0.1606, 0.1656],\n",
       "        [0.1891, 0.1416, 0.1830, 0.1609, 0.1602, 0.1652],\n",
       "        [0.1887, 0.1415, 0.1831, 0.1609, 0.1605, 0.1653],\n",
       "        [0.1892, 0.1413, 0.1831, 0.1610, 0.1603, 0.1651],\n",
       "        [0.1894, 0.1415, 0.1834, 0.1607, 0.1598, 0.1651],\n",
       "        [0.1894, 0.1412, 0.1825, 0.1609, 0.1604, 0.1656],\n",
       "        [0.1890, 0.1415, 0.1829, 0.1609, 0.1603, 0.1654],\n",
       "        [0.1892, 0.1412, 0.1828, 0.1608, 0.1604, 0.1655],\n",
       "        [0.1890, 0.1417, 0.1827, 0.1609, 0.1603, 0.1654],\n",
       "        [0.1888, 0.1415, 0.1827, 0.1609, 0.1607, 0.1654],\n",
       "        [0.1892, 0.1414, 0.1830, 0.1610, 0.1600, 0.1653],\n",
       "        [0.1894, 0.1414, 0.1828, 0.1611, 0.1601, 0.1652],\n",
       "        [0.1891, 0.1416, 0.1825, 0.1609, 0.1603, 0.1654],\n",
       "        [0.1894, 0.1414, 0.1830, 0.1611, 0.1599, 0.1652],\n",
       "        [0.1889, 0.1416, 0.1829, 0.1609, 0.1604, 0.1652],\n",
       "        [0.1890, 0.1414, 0.1831, 0.1604, 0.1606, 0.1654],\n",
       "        [0.1890, 0.1416, 0.1833, 0.1610, 0.1599, 0.1651],\n",
       "        [0.1890, 0.1414, 0.1829, 0.1605, 0.1606, 0.1656],\n",
       "        [0.1894, 0.1411, 0.1825, 0.1610, 0.1605, 0.1654],\n",
       "        [0.1889, 0.1414, 0.1833, 0.1608, 0.1604, 0.1651],\n",
       "        [0.1895, 0.1412, 0.1829, 0.1609, 0.1601, 0.1654],\n",
       "        [0.1891, 0.1417, 0.1830, 0.1607, 0.1602, 0.1653],\n",
       "        [0.1892, 0.1415, 0.1829, 0.1608, 0.1601, 0.1654],\n",
       "        [0.1887, 0.1414, 0.1831, 0.1606, 0.1608, 0.1654],\n",
       "        [0.1894, 0.1414, 0.1834, 0.1608, 0.1599, 0.1651],\n",
       "        [0.1892, 0.1412, 0.1828, 0.1610, 0.1604, 0.1653],\n",
       "        [0.1889, 0.1416, 0.1836, 0.1609, 0.1600, 0.1649],\n",
       "        [0.1889, 0.1415, 0.1833, 0.1606, 0.1604, 0.1653],\n",
       "        [0.1889, 0.1414, 0.1833, 0.1606, 0.1604, 0.1655],\n",
       "        [0.1890, 0.1418, 0.1832, 0.1608, 0.1599, 0.1653],\n",
       "        [0.1895, 0.1411, 0.1824, 0.1611, 0.1603, 0.1656],\n",
       "        [0.1893, 0.1412, 0.1830, 0.1608, 0.1603, 0.1654],\n",
       "        [0.1892, 0.1414, 0.1829, 0.1607, 0.1603, 0.1655],\n",
       "        [0.1892, 0.1415, 0.1828, 0.1609, 0.1604, 0.1652],\n",
       "        [0.1890, 0.1415, 0.1826, 0.1611, 0.1602, 0.1655],\n",
       "        [0.1890, 0.1415, 0.1831, 0.1605, 0.1608, 0.1651],\n",
       "        [0.1889, 0.1415, 0.1832, 0.1606, 0.1604, 0.1653],\n",
       "        [0.1893, 0.1415, 0.1832, 0.1610, 0.1600, 0.1650],\n",
       "        [0.1891, 0.1414, 0.1831, 0.1609, 0.1602, 0.1653],\n",
       "        [0.1889, 0.1415, 0.1831, 0.1609, 0.1605, 0.1652],\n",
       "        [0.1893, 0.1413, 0.1826, 0.1609, 0.1605, 0.1655],\n",
       "        [0.1890, 0.1414, 0.1834, 0.1610, 0.1602, 0.1650],\n",
       "        [0.1893, 0.1413, 0.1831, 0.1607, 0.1602, 0.1653],\n",
       "        [0.1895, 0.1411, 0.1827, 0.1610, 0.1604, 0.1655]], device='cuda:0',\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "model(input_feats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
