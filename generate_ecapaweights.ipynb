{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary libraries and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_lib.modules import *\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from python_lib.saveasfile import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model initialisation \n",
    "\n",
    "Use BlockSave to save all the weights into a `.bin` file\n",
    "\n",
    "Be sure to `load_state_dict` from a `.pt`/`.pth` file before BlockSave\n",
    "\n",
    "It's omitted here as I don't have the weights on hand, so I just use a random initialised weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<python_lib.saveasfile.BlockSave at 0x1e4bae87d30>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ECAPA_TDNN(input_size = 2, channels=[8,8,8,8,16], lin_neurons=6, device = \"cuda\").to(\"cuda\")\n",
    "\n",
    "BlockSave(model.return_layers(), \"fullecapa\", \"ECAPAweights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Input Feats for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.34149832 0.37636566 0.62650234 ... 0.09052432 0.76767457 0.23272258]\n",
      "  [0.53362054 0.48166353 0.79475635 ... 0.150886   0.7996159  0.12034094]]\n",
      "\n",
      " [[0.59842473 0.02847058 0.79718274 ... 0.46615368 0.30636483 0.58332264]\n",
      "  [0.98534065 0.31697178 0.24182647 ... 0.93761045 0.45935005 0.7261669 ]]\n",
      "\n",
      " [[0.807831   0.09603226 0.5602161  ... 0.57906103 0.34993565 0.5375221 ]\n",
      "  [0.8633781  0.68299186 0.06199419 ... 0.6680984  0.07415938 0.19900131]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.32583755 0.31459486 0.6805412  ... 0.86272275 0.1265719  0.8649771 ]\n",
      "  [0.21651435 0.44576806 0.20779884 ... 0.793362   0.30055386 0.28084207]]\n",
      "\n",
      " [[0.9380531  0.09069198 0.95391923 ... 0.12276822 0.9166646  0.08865559]\n",
      "  [0.64113635 0.91034216 0.9125979  ... 0.22270381 0.29946    0.9769074 ]]\n",
      "\n",
      " [[0.03444189 0.2212426  0.73028284 ... 0.01019555 0.49794346 0.35037827]\n",
      "  [0.4887898  0.04597694 0.7009779  ... 0.20657468 0.55839556 0.7299678 ]]]\n",
      "(100, 2, 64)\n"
     ]
    }
   ],
   "source": [
    "input_feats = torch.rand([100,2,64]).to(\"cuda\")\n",
    "input_feats_np = input_feats.cpu().detach().numpy()\n",
    "input_feats_np = np.reshape(input_feats_np, (100,2,64))\n",
    "print(input_feats_np)\n",
    "dim = input_feats_np.shape\n",
    "print(dim)\n",
    "\n",
    "flatten_inputs = input_feats_np.flatten()\n",
    "with open(os.path.join(\"ECAPAweights\", f\"ecapainput_2x64_100.bin\"), \"wb\") as f:\n",
    "            # Write the dimensions down\n",
    "            f.write(np.array(dim, dtype=np.int32).tobytes())\n",
    "            # Write the flatten bias down\n",
    "            f.write(flatten_inputs.tobytes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the model using eval/train mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1682, 0.1453, 0.1962, 0.1661, 0.1516, 0.1725],\n",
       "        [0.1683, 0.1452, 0.1959, 0.1658, 0.1515, 0.1734],\n",
       "        [0.1680, 0.1452, 0.1958, 0.1662, 0.1516, 0.1731],\n",
       "        [0.1679, 0.1454, 0.1956, 0.1661, 0.1515, 0.1735],\n",
       "        [0.1684, 0.1451, 0.1968, 0.1660, 0.1519, 0.1718],\n",
       "        [0.1680, 0.1453, 0.1959, 0.1663, 0.1516, 0.1729],\n",
       "        [0.1679, 0.1453, 0.1961, 0.1664, 0.1517, 0.1727],\n",
       "        [0.1679, 0.1451, 0.1964, 0.1663, 0.1518, 0.1725],\n",
       "        [0.1681, 0.1453, 0.1961, 0.1661, 0.1516, 0.1728],\n",
       "        [0.1687, 0.1448, 0.1972, 0.1654, 0.1518, 0.1720],\n",
       "        [0.1686, 0.1450, 0.1968, 0.1654, 0.1517, 0.1724],\n",
       "        [0.1680, 0.1452, 0.1962, 0.1663, 0.1518, 0.1725],\n",
       "        [0.1677, 0.1455, 0.1953, 0.1665, 0.1515, 0.1736],\n",
       "        [0.1682, 0.1450, 0.1966, 0.1662, 0.1519, 0.1720],\n",
       "        [0.1686, 0.1448, 0.1971, 0.1658, 0.1519, 0.1717],\n",
       "        [0.1689, 0.1447, 0.1973, 0.1654, 0.1519, 0.1719],\n",
       "        [0.1682, 0.1449, 0.1973, 0.1659, 0.1519, 0.1717],\n",
       "        [0.1690, 0.1450, 0.1963, 0.1652, 0.1515, 0.1729],\n",
       "        [0.1680, 0.1453, 0.1961, 0.1661, 0.1516, 0.1729],\n",
       "        [0.1684, 0.1449, 0.1967, 0.1661, 0.1518, 0.1722],\n",
       "        [0.1682, 0.1452, 0.1965, 0.1659, 0.1518, 0.1723],\n",
       "        [0.1682, 0.1450, 0.1965, 0.1659, 0.1516, 0.1728],\n",
       "        [0.1684, 0.1450, 0.1965, 0.1658, 0.1517, 0.1726],\n",
       "        [0.1684, 0.1450, 0.1964, 0.1658, 0.1516, 0.1727],\n",
       "        [0.1683, 0.1451, 0.1963, 0.1660, 0.1516, 0.1727],\n",
       "        [0.1681, 0.1451, 0.1967, 0.1661, 0.1519, 0.1719],\n",
       "        [0.1689, 0.1447, 0.1975, 0.1648, 0.1517, 0.1725],\n",
       "        [0.1679, 0.1453, 0.1959, 0.1664, 0.1517, 0.1728],\n",
       "        [0.1681, 0.1451, 0.1967, 0.1659, 0.1518, 0.1724],\n",
       "        [0.1682, 0.1451, 0.1967, 0.1659, 0.1517, 0.1723],\n",
       "        [0.1687, 0.1451, 0.1969, 0.1654, 0.1517, 0.1721],\n",
       "        [0.1686, 0.1450, 0.1969, 0.1656, 0.1518, 0.1721],\n",
       "        [0.1682, 0.1453, 0.1957, 0.1663, 0.1516, 0.1729],\n",
       "        [0.1683, 0.1451, 0.1961, 0.1660, 0.1516, 0.1729],\n",
       "        [0.1680, 0.1453, 0.1959, 0.1661, 0.1515, 0.1732],\n",
       "        [0.1681, 0.1450, 0.1967, 0.1664, 0.1519, 0.1719],\n",
       "        [0.1684, 0.1451, 0.1962, 0.1659, 0.1515, 0.1729],\n",
       "        [0.1684, 0.1449, 0.1975, 0.1655, 0.1519, 0.1718],\n",
       "        [0.1675, 0.1455, 0.1954, 0.1669, 0.1516, 0.1731],\n",
       "        [0.1683, 0.1448, 0.1969, 0.1663, 0.1520, 0.1717],\n",
       "        [0.1683, 0.1449, 0.1971, 0.1659, 0.1519, 0.1718],\n",
       "        [0.1689, 0.1449, 0.1970, 0.1653, 0.1518, 0.1721],\n",
       "        [0.1689, 0.1448, 0.1974, 0.1652, 0.1518, 0.1720],\n",
       "        [0.1685, 0.1450, 0.1969, 0.1657, 0.1517, 0.1722],\n",
       "        [0.1677, 0.1453, 0.1959, 0.1666, 0.1517, 0.1728],\n",
       "        [0.1682, 0.1452, 0.1964, 0.1660, 0.1517, 0.1725],\n",
       "        [0.1679, 0.1452, 0.1960, 0.1664, 0.1517, 0.1729],\n",
       "        [0.1684, 0.1451, 0.1961, 0.1659, 0.1516, 0.1728],\n",
       "        [0.1677, 0.1452, 0.1965, 0.1667, 0.1520, 0.1720],\n",
       "        [0.1682, 0.1449, 0.1968, 0.1661, 0.1518, 0.1722],\n",
       "        [0.1689, 0.1447, 0.1975, 0.1655, 0.1519, 0.1716],\n",
       "        [0.1686, 0.1448, 0.1965, 0.1658, 0.1517, 0.1725],\n",
       "        [0.1680, 0.1451, 0.1966, 0.1662, 0.1518, 0.1723],\n",
       "        [0.1686, 0.1450, 0.1967, 0.1659, 0.1518, 0.1720],\n",
       "        [0.1679, 0.1452, 0.1959, 0.1662, 0.1516, 0.1733],\n",
       "        [0.1680, 0.1452, 0.1962, 0.1662, 0.1517, 0.1726],\n",
       "        [0.1684, 0.1448, 0.1972, 0.1657, 0.1518, 0.1720],\n",
       "        [0.1690, 0.1448, 0.1968, 0.1653, 0.1516, 0.1725],\n",
       "        [0.1680, 0.1452, 0.1964, 0.1662, 0.1517, 0.1725],\n",
       "        [0.1687, 0.1451, 0.1968, 0.1656, 0.1517, 0.1722],\n",
       "        [0.1678, 0.1453, 0.1960, 0.1665, 0.1516, 0.1728],\n",
       "        [0.1680, 0.1452, 0.1960, 0.1664, 0.1517, 0.1727],\n",
       "        [0.1690, 0.1448, 0.1967, 0.1652, 0.1516, 0.1727],\n",
       "        [0.1675, 0.1452, 0.1957, 0.1668, 0.1517, 0.1731],\n",
       "        [0.1678, 0.1453, 0.1956, 0.1663, 0.1514, 0.1736],\n",
       "        [0.1680, 0.1452, 0.1961, 0.1663, 0.1517, 0.1727],\n",
       "        [0.1680, 0.1451, 0.1964, 0.1665, 0.1519, 0.1720],\n",
       "        [0.1681, 0.1452, 0.1960, 0.1664, 0.1517, 0.1726],\n",
       "        [0.1681, 0.1452, 0.1957, 0.1664, 0.1516, 0.1730],\n",
       "        [0.1680, 0.1450, 0.1966, 0.1663, 0.1519, 0.1723],\n",
       "        [0.1683, 0.1449, 0.1967, 0.1660, 0.1518, 0.1723],\n",
       "        [0.1676, 0.1454, 0.1955, 0.1667, 0.1515, 0.1732],\n",
       "        [0.1680, 0.1453, 0.1959, 0.1663, 0.1515, 0.1730],\n",
       "        [0.1681, 0.1450, 0.1970, 0.1660, 0.1519, 0.1719],\n",
       "        [0.1681, 0.1450, 0.1972, 0.1663, 0.1521, 0.1713],\n",
       "        [0.1683, 0.1453, 0.1962, 0.1659, 0.1517, 0.1726],\n",
       "        [0.1688, 0.1450, 0.1967, 0.1654, 0.1516, 0.1725],\n",
       "        [0.1684, 0.1450, 0.1967, 0.1658, 0.1518, 0.1723],\n",
       "        [0.1685, 0.1451, 0.1962, 0.1659, 0.1517, 0.1727],\n",
       "        [0.1682, 0.1450, 0.1966, 0.1659, 0.1518, 0.1725],\n",
       "        [0.1681, 0.1452, 0.1966, 0.1662, 0.1518, 0.1721],\n",
       "        [0.1679, 0.1452, 0.1963, 0.1663, 0.1517, 0.1726],\n",
       "        [0.1677, 0.1455, 0.1951, 0.1667, 0.1514, 0.1736],\n",
       "        [0.1682, 0.1451, 0.1963, 0.1660, 0.1517, 0.1727],\n",
       "        [0.1682, 0.1451, 0.1963, 0.1660, 0.1517, 0.1727],\n",
       "        [0.1681, 0.1452, 0.1965, 0.1662, 0.1518, 0.1723],\n",
       "        [0.1678, 0.1452, 0.1956, 0.1663, 0.1514, 0.1736],\n",
       "        [0.1684, 0.1450, 0.1970, 0.1658, 0.1519, 0.1719],\n",
       "        [0.1681, 0.1450, 0.1966, 0.1661, 0.1518, 0.1724],\n",
       "        [0.1683, 0.1450, 0.1965, 0.1659, 0.1517, 0.1725],\n",
       "        [0.1679, 0.1453, 0.1961, 0.1664, 0.1517, 0.1726],\n",
       "        [0.1689, 0.1450, 0.1964, 0.1652, 0.1515, 0.1730],\n",
       "        [0.1682, 0.1450, 0.1966, 0.1659, 0.1518, 0.1725],\n",
       "        [0.1679, 0.1454, 0.1958, 0.1664, 0.1516, 0.1729],\n",
       "        [0.1672, 0.1456, 0.1951, 0.1670, 0.1514, 0.1738],\n",
       "        [0.1682, 0.1452, 0.1960, 0.1662, 0.1517, 0.1727],\n",
       "        [0.1684, 0.1451, 0.1966, 0.1658, 0.1517, 0.1724],\n",
       "        [0.1676, 0.1452, 0.1961, 0.1666, 0.1517, 0.1727],\n",
       "        [0.1685, 0.1450, 0.1963, 0.1658, 0.1517, 0.1726],\n",
       "        [0.1686, 0.1447, 0.1969, 0.1658, 0.1518, 0.1721]], device='cuda:0',\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "model(input_feats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
