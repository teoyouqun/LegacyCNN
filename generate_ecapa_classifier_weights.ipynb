{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chiny\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\chiny\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n",
      "SpeechBrain could not find any working torchaudio backend. Audio files may fail to load. Follow this link for instructions and troubleshooting: https://pytorch.org/audio/stable/index.html\n",
      "SpeechBrain could not find any working torchaudio backend. Audio files may fail to load. Follow this link for instructions and troubleshooting: https://pytorch.org/audio/stable/index.html\n"
     ]
    }
   ],
   "source": [
    "from python_lib.ecapa_classifier import *\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from python_lib.saveasfile import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<python_lib.saveasfile.BlockSave at 0x12b5200c9d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This out_neurons is after fc. Hence, due to ASP previously, we set out_neuron = 1\n",
    "\n",
    "# metrics_type = cosine / cdist / euclidean\n",
    "\n",
    "model = ECAPA_TDNN(input_size = 2, \n",
    "                   channels=[8,8,8,8,16], \n",
    "                   lin_neurons=6, \n",
    "                   device = \"cuda\", \n",
    "                   out_neurons=1, \n",
    "                   metrics_type=\"cosine\").to(\"cuda\")\n",
    "\n",
    "BlockSave(model.return_layers(), \"fullecapa_classifier\", \"ECAPAweights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Input Feats for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.27643448 0.6296002  0.21727002 0.29030764 0.73350686 0.8506823\n",
      "  0.46465695 0.7218458  0.60782856 0.03799403 0.08702713 0.85264975\n",
      "  0.81630635 0.47291422 0.31429732 0.04622483 0.4177739  0.74881077\n",
      "  0.38275838 0.78756183 0.5641921  0.62360156 0.6026761  0.6052883\n",
      "  0.45323956 0.5475703  0.23846811 0.8575705  0.34887445 0.14909357\n",
      "  0.9909906  0.16943175 0.06554288 0.1186102  0.00921285 0.5941841\n",
      "  0.4188913  0.5894744  0.4782551  0.5332605  0.8347122  0.46870202\n",
      "  0.72033155 0.53493285 0.4297642  0.7862897  0.9295213  0.07271159\n",
      "  0.5772995  0.7669005  0.08591104 0.6189596  0.1881324  0.7188026\n",
      "  0.82613075 0.9525146  0.7991411  0.79186904 0.48795044 0.5225593\n",
      "  0.4717747  0.7344063  0.98081386 0.63737035]\n",
      " [0.10761833 0.5649239  0.53854305 0.5417653  0.597723   0.5166261\n",
      "  0.25423145 0.0539555  0.9480375  0.39726388 0.7437156  0.5806329\n",
      "  0.27280593 0.9835242  0.67378485 0.46670294 0.66185284 0.04554272\n",
      "  0.56261307 0.3894056  0.7841782  0.06404054 0.1341123  0.39458466\n",
      "  0.12876004 0.72394425 0.520187   0.2279824  0.00686872 0.09833324\n",
      "  0.7650348  0.26410758 0.4823928  0.78932416 0.5043551  0.7885728\n",
      "  0.28345203 0.53207755 0.5261027  0.59337467 0.11683458 0.5172711\n",
      "  0.01662076 0.391019   0.52459747 0.28807813 0.07423633 0.5796537\n",
      "  0.13486254 0.36108464 0.66889536 0.00146389 0.17254645 0.45619977\n",
      "  0.5207407  0.5483657  0.7044598  0.50536615 0.7776684  0.07419455\n",
      "  0.13021952 0.9655959  0.6215041  0.75762117]]\n",
      "(2, 64)\n"
     ]
    }
   ],
   "source": [
    "input_feats = torch.rand([1,2,64]).to(\"cuda\")\n",
    "input_feats_np = input_feats.cpu().detach().numpy()\n",
    "input_feats_np = np.reshape(input_feats_np, (2,64))\n",
    "print(input_feats_np)\n",
    "dim = input_feats_np.shape\n",
    "print(dim)\n",
    "\n",
    "flatten_inputs = input_feats_np.flatten()\n",
    "with open(os.path.join(\"ECAPAweights\", f\"ecapainput_2x64.bin\"), \"wb\") as f:\n",
    "            # Write the dimensions down\n",
    "            f.write(np.array(dim, dtype=np.int32).tobytes())\n",
    "            # Write the flatten bias down\n",
    "            f.write(flatten_inputs.tobytes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the model using eval/train mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          5.4281e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [1.3545e-01, 1.6933e-01, 0.0000e+00, 0.0000e+00, 1.9168e-01,\n",
      "          1.5448e-01, 0.0000e+00, 1.7703e-01, 2.0756e-01, 0.0000e+00,\n",
      "          0.0000e+00, 2.6532e-01, 5.1564e-01, 2.1349e-01, 4.5274e-02,\n",
      "          7.0140e-02, 6.8364e-02, 1.9545e-01, 7.9332e-02, 0.0000e+00,\n",
      "          3.8467e-02, 0.0000e+00, 6.9720e-02, 0.0000e+00, 3.1586e-03,\n",
      "          0.0000e+00, 0.0000e+00, 1.6029e-01, 0.0000e+00, 0.0000e+00,\n",
      "          1.6195e-01, 1.2631e-01, 8.9339e-02, 6.1464e-02, 8.9088e-02,\n",
      "          2.5610e-01, 2.0020e-01, 1.8764e-01, 0.0000e+00, 8.2011e-03,\n",
      "          1.0385e-01, 0.0000e+00, 0.0000e+00, 2.0782e-02, 0.0000e+00,\n",
      "          7.6699e-02, 2.1506e-01, 0.0000e+00, 0.0000e+00, 2.5631e-01,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1171e-01,\n",
      "          1.6211e-01, 1.8017e-01, 1.4831e-01, 1.4262e-02, 0.0000e+00,\n",
      "          1.0750e-01, 2.7207e-02, 1.7788e-01, 2.8206e-01],\n",
      "         [8.1187e-03, 1.0766e-03, 0.0000e+00, 5.8896e-02, 2.8386e-02,\n",
      "          5.3435e-04, 5.5682e-02, 5.5261e-02, 0.0000e+00, 0.0000e+00,\n",
      "          1.9714e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          1.6839e-01, 1.9305e-01, 3.9559e-02, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 1.9076e-01, 2.0205e-01, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 2.4064e-02, 3.3992e-01, 1.6668e-02, 1.1209e-01,\n",
      "          0.0000e+00, 0.0000e+00, 8.9351e-02, 0.0000e+00, 0.0000e+00,\n",
      "          8.7057e-02, 0.0000e+00, 3.8820e-02, 0.0000e+00, 8.5093e-02,\n",
      "          1.2630e-01, 5.3903e-02, 7.8911e-02, 0.0000e+00, 0.0000e+00,\n",
      "          1.4795e-01, 0.0000e+00, 0.0000e+00, 1.6302e-01, 0.0000e+00,\n",
      "          0.0000e+00, 2.7825e-01, 5.0158e-02, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 5.7618e-02, 1.4594e-01,\n",
      "          1.6548e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          7.8013e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5798e-02, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 1.5267e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.3508e-02,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.2666e-03,\n",
      "          0.0000e+00, 0.0000e+00, 3.3490e-02, 0.0000e+00, 0.0000e+00,\n",
      "          1.3829e-02, 0.0000e+00, 7.0189e-02, 0.0000e+00, 1.8022e-02,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 8.6549e-02, 0.0000e+00, 0.0000e+00],\n",
      "         [3.1744e-01, 1.9708e-01, 2.7494e-01, 5.5398e-01, 3.7519e-01,\n",
      "          1.0644e-01, 3.1250e-01, 1.3231e-01, 9.3251e-03, 2.5077e-01,\n",
      "          7.1505e-01, 5.1818e-01, 3.0786e-01, 1.0464e-01, 1.7396e-01,\n",
      "          5.1079e-01, 4.8242e-01, 2.8731e-01, 4.2928e-01, 8.1554e-02,\n",
      "          1.6824e-01, 1.3357e-01, 1.6858e-01, 8.3241e-02, 1.2508e-01,\n",
      "          1.9436e-01, 1.6730e-01, 3.1044e-02, 2.3573e-01, 2.6346e-01,\n",
      "          0.0000e+00, 5.5612e-02, 2.9044e-01, 4.3264e-01, 5.4561e-01,\n",
      "          5.0501e-01, 3.7592e-01, 3.6996e-01, 2.3424e-01, 3.2451e-01,\n",
      "          1.2176e-01, 1.9241e-01, 5.2418e-02, 2.0073e-01, 1.7317e-01,\n",
      "          1.7847e-01, 0.0000e+00, 1.6523e-01, 1.8168e-01, 8.6346e-02,\n",
      "          3.9822e-02, 1.6214e-01, 4.9276e-01, 3.3985e-01, 3.5860e-01,\n",
      "          2.5389e-01, 2.4581e-01, 6.9069e-02, 1.1979e-01, 3.2547e-01,\n",
      "          5.6271e-01, 3.8310e-01, 2.7670e-01, 5.6838e-01],\n",
      "         [1.1645e-01, 1.2332e-01, 2.3066e-01, 2.5814e-01, 2.5580e-01,\n",
      "          5.3824e-02, 1.4940e-01, 0.0000e+00, 2.1549e-01, 2.4189e-01,\n",
      "          2.7610e-01, 5.1614e-01, 0.0000e+00, 1.5953e-01, 4.0866e-01,\n",
      "          1.1207e-01, 4.5554e-01, 2.8415e-02, 8.2100e-02, 1.0973e-01,\n",
      "          1.6314e-01, 1.5002e-01, 0.0000e+00, 1.6088e-01, 7.6305e-02,\n",
      "          5.5042e-02, 3.5174e-01, 1.5145e-01, 0.0000e+00, 5.6102e-02,\n",
      "          3.2858e-01, 2.0419e-01, 1.6849e-01, 4.4730e-01, 3.7297e-01,\n",
      "          3.7700e-01, 2.0079e-01, 9.9678e-02, 1.0282e-01, 3.1538e-01,\n",
      "          0.0000e+00, 6.3894e-02, 1.1134e-01, 0.0000e+00, 1.7802e-01,\n",
      "          3.0735e-01, 0.0000e+00, 6.3166e-02, 3.8274e-01, 0.0000e+00,\n",
      "          3.0821e-01, 1.8192e-01, 0.0000e+00, 1.9800e-01, 1.7150e-01,\n",
      "          2.4561e-02, 1.5689e-01, 0.0000e+00, 1.6918e-01, 2.1959e-01,\n",
      "          0.0000e+00, 3.1750e-01, 2.1306e-01, 1.3323e-01],\n",
      "         [1.5665e-01, 1.4155e-01, 4.1408e-02, 0.0000e+00, 2.3176e-01,\n",
      "          7.9107e-02, 4.9287e-02, 7.5794e-02, 3.1090e-01, 0.0000e+00,\n",
      "          0.0000e+00, 4.6727e-01, 4.0487e-01, 2.4187e-01, 1.4900e-01,\n",
      "          0.0000e+00, 2.2007e-01, 9.3380e-02, 1.1371e-01, 0.0000e+00,\n",
      "          6.6327e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7318e-02,\n",
      "          0.0000e+00, 0.0000e+00, 1.1336e-01, 0.0000e+00, 0.0000e+00,\n",
      "          2.9449e-01, 2.3880e-01, 5.3363e-02, 1.9527e-01, 1.6677e-01,\n",
      "          3.0570e-01, 2.3422e-01, 1.3540e-01, 0.0000e+00, 1.2089e-01,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          2.2303e-01, 0.0000e+00, 0.0000e+00, 5.5806e-02, 5.0114e-02,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9344e-01,\n",
      "          1.0564e-01, 2.3057e-01, 2.1411e-02, 1.8259e-02, 1.0831e-02,\n",
      "          0.0000e+00, 1.9774e-01, 2.0212e-01, 2.8438e-01],\n",
      "         [1.1516e-01, 1.2181e-01, 1.2798e-02, 3.6507e-02, 2.5217e-01,\n",
      "          2.3329e-01, 0.0000e+00, 1.9338e-01, 1.3302e-01, 5.7290e-02,\n",
      "          1.6729e-01, 1.3223e-02, 3.2477e-01, 3.5725e-01, 1.9784e-01,\n",
      "          3.2925e-01, 4.9741e-02, 9.1397e-02, 0.0000e+00, 1.9098e-01,\n",
      "          1.6252e-01, 0.0000e+00, 7.7401e-02, 0.0000e+00, 3.4941e-02,\n",
      "          8.7024e-02, 1.6690e-01, 2.1034e-01, 0.0000e+00, 0.0000e+00,\n",
      "          1.0970e-01, 9.9579e-02, 2.4466e-01, 5.4579e-02, 3.1041e-01,\n",
      "          1.9205e-01, 1.7064e-01, 1.0794e-01, 1.5701e-01, 2.9401e-02,\n",
      "          1.8851e-01, 0.0000e+00, 0.0000e+00, 5.6341e-03, 0.0000e+00,\n",
      "          6.3943e-03, 1.7416e-01, 0.0000e+00, 0.0000e+00, 2.7203e-01,\n",
      "          5.2942e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          1.2595e-01, 1.0578e-01, 3.7711e-01, 2.2180e-01, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 1.5442e-01, 1.1925e-01]]], device='cuda:0',\n",
      "       grad_fn=<CudnnBatchNormBackward0>)\n",
      "tensor([[[1.2724e-01, 1.3042e-01, 1.2499e-01, 1.2213e-01, 1.2892e-01,\n",
      "          1.2497e-01, 1.2299e-01, 1.2887e-01, 1.3320e-01, 1.2860e-01,\n",
      "          1.2194e-01, 1.2663e-01, 1.3064e-01, 1.2907e-01, 1.3326e-01,\n",
      "          1.2605e-01, 1.2753e-01, 1.2448e-01, 1.2234e-01, 1.2780e-01,\n",
      "          1.2680e-01, 1.2655e-01, 1.2480e-01, 1.2377e-01, 1.2628e-01,\n",
      "          1.2578e-01, 1.2651e-01, 1.3026e-01, 1.2309e-01, 1.2236e-01,\n",
      "          1.3030e-01, 1.3073e-01, 1.2706e-01, 1.2964e-01, 1.2691e-01,\n",
      "          1.2899e-01, 1.2656e-01, 1.2789e-01, 1.2416e-01, 1.2621e-01,\n",
      "          1.2698e-01, 1.2306e-01, 1.2660e-01, 1.2576e-01, 1.2408e-01,\n",
      "          1.2783e-01, 1.3115e-01, 1.2235e-01, 1.2854e-01, 1.3131e-01,\n",
      "          1.8056e-01, 1.2790e-01, 1.2392e-01, 1.2232e-01, 1.2523e-01,\n",
      "          1.2232e-01, 1.2673e-01, 1.3046e-01, 1.2785e-01, 1.2703e-01,\n",
      "          1.2386e-01, 1.2571e-01, 1.2732e-01, 1.2797e-01],\n",
      "         [2.0045e-01, 2.3931e-01, 5.9981e-02, 5.4824e-02, 2.6505e-01,\n",
      "          2.2149e-01, 5.6714e-02, 2.4490e-01, 2.8843e-01, 6.2313e-02,\n",
      "          5.6474e-02, 3.4543e-01, 5.9946e-01, 2.9087e-01, 1.2073e-01,\n",
      "          1.3041e-01, 1.3943e-01, 2.5910e-01, 1.3538e-01, 6.2648e-02,\n",
      "          1.0265e-01, 6.2507e-02, 1.3034e-01, 6.0343e-02, 6.2294e-02,\n",
      "          5.7130e-02, 6.7583e-02, 2.3797e-01, 5.4327e-02, 5.4759e-02,\n",
      "          2.4256e-01, 2.0042e-01, 1.5531e-01, 1.3147e-01, 1.5780e-01,\n",
      "          3.3409e-01, 2.7020e-01, 2.5366e-01, 5.7717e-02, 7.2588e-02,\n",
      "          1.6795e-01, 5.5931e-02, 6.1665e-02, 7.6307e-02, 5.9453e-02,\n",
      "          1.4869e-01, 2.8620e-01, 5.6043e-02, 6.9391e-02, 3.2920e-01,\n",
      "          6.7947e-02, 6.4467e-02, 5.4701e-02, 5.6081e-02, 1.7513e-01,\n",
      "          2.2398e-01, 2.4988e-01, 2.1744e-01, 7.9651e-02, 6.0817e-02,\n",
      "          1.6217e-01, 9.2248e-02, 2.4892e-01, 3.5073e-01],\n",
      "         [8.1187e-03, 1.0766e-03, 0.0000e+00, 5.8896e-02, 2.8386e-02,\n",
      "          5.3435e-04, 5.5682e-02, 5.5261e-02, 0.0000e+00, 0.0000e+00,\n",
      "          1.9714e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          1.6839e-01, 1.9305e-01, 3.9559e-02, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 1.9076e-01, 2.0205e-01, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 2.4064e-02, 3.3992e-01, 1.6668e-02, 1.1209e-01,\n",
      "          0.0000e+00, 0.0000e+00, 8.9351e-02, 0.0000e+00, 0.0000e+00,\n",
      "          8.7057e-02, 0.0000e+00, 3.8820e-02, 0.0000e+00, 8.5093e-02,\n",
      "          1.2630e-01, 5.3903e-02, 7.8911e-02, 0.0000e+00, 0.0000e+00,\n",
      "          1.4795e-01, 0.0000e+00, 0.0000e+00, 1.6302e-01, 0.0000e+00,\n",
      "          0.0000e+00, 2.7825e-01, 5.0158e-02, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 5.7618e-02, 1.4594e-01,\n",
      "          1.6548e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          7.8013e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5798e-02, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 1.5267e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.3508e-02,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.2666e-03,\n",
      "          0.0000e+00, 0.0000e+00, 3.3490e-02, 0.0000e+00, 0.0000e+00,\n",
      "          1.3829e-02, 0.0000e+00, 7.0189e-02, 0.0000e+00, 1.8022e-02,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 8.6549e-02, 0.0000e+00, 0.0000e+00],\n",
      "         [3.1744e-01, 1.9708e-01, 2.7494e-01, 5.5398e-01, 3.7519e-01,\n",
      "          1.0644e-01, 3.1250e-01, 1.3231e-01, 9.3251e-03, 2.5077e-01,\n",
      "          7.1505e-01, 5.1818e-01, 3.0786e-01, 1.0464e-01, 1.7396e-01,\n",
      "          5.1079e-01, 4.8242e-01, 2.8731e-01, 4.2928e-01, 8.1554e-02,\n",
      "          1.6824e-01, 1.3357e-01, 1.6858e-01, 8.3241e-02, 1.2508e-01,\n",
      "          1.9436e-01, 1.6730e-01, 3.1044e-02, 2.3573e-01, 2.6346e-01,\n",
      "          0.0000e+00, 5.5612e-02, 2.9044e-01, 4.3264e-01, 5.4561e-01,\n",
      "          5.0501e-01, 3.7592e-01, 3.6996e-01, 2.3424e-01, 3.2451e-01,\n",
      "          1.2176e-01, 1.9241e-01, 5.2418e-02, 2.0073e-01, 1.7317e-01,\n",
      "          1.7847e-01, 0.0000e+00, 1.6523e-01, 1.8168e-01, 8.6346e-02,\n",
      "          3.9822e-02, 1.6214e-01, 4.9276e-01, 3.3985e-01, 3.5860e-01,\n",
      "          2.5389e-01, 2.4581e-01, 6.9069e-02, 1.1979e-01, 3.2547e-01,\n",
      "          5.6271e-01, 3.8310e-01, 2.7670e-01, 5.6838e-01],\n",
      "         [1.1645e-01, 1.2332e-01, 2.3066e-01, 2.5814e-01, 2.5580e-01,\n",
      "          5.3824e-02, 1.4940e-01, 0.0000e+00, 2.1549e-01, 2.4189e-01,\n",
      "          2.7610e-01, 5.1614e-01, 0.0000e+00, 1.5953e-01, 4.0866e-01,\n",
      "          1.1207e-01, 4.5554e-01, 2.8415e-02, 8.2100e-02, 1.0973e-01,\n",
      "          1.6314e-01, 1.5002e-01, 0.0000e+00, 1.6088e-01, 7.6305e-02,\n",
      "          5.5042e-02, 3.5174e-01, 1.5145e-01, 0.0000e+00, 5.6102e-02,\n",
      "          3.2858e-01, 2.0419e-01, 1.6849e-01, 4.4730e-01, 3.7297e-01,\n",
      "          3.7700e-01, 2.0079e-01, 9.9678e-02, 1.0282e-01, 3.1538e-01,\n",
      "          0.0000e+00, 6.3894e-02, 1.1134e-01, 0.0000e+00, 1.7802e-01,\n",
      "          3.0735e-01, 0.0000e+00, 6.3166e-02, 3.8274e-01, 0.0000e+00,\n",
      "          3.0821e-01, 1.8192e-01, 0.0000e+00, 1.9800e-01, 1.7150e-01,\n",
      "          2.4561e-02, 1.5689e-01, 0.0000e+00, 1.6918e-01, 2.1959e-01,\n",
      "          0.0000e+00, 3.1750e-01, 2.1306e-01, 1.3323e-01],\n",
      "         [2.9143e-01, 2.8436e-01, 1.6929e-01, 1.2012e-01, 3.7550e-01,\n",
      "          2.1271e-01, 1.7201e-01, 2.1496e-01, 4.6608e-01, 1.3427e-01,\n",
      "          1.2122e-01, 6.1367e-01, 5.5929e-01, 3.8907e-01, 2.9982e-01,\n",
      "          1.2944e-01, 3.6019e-01, 2.2364e-01, 2.3510e-01, 1.3356e-01,\n",
      "          1.9990e-01, 1.3189e-01, 1.2818e-01, 1.2667e-01, 1.5611e-01,\n",
      "          1.2653e-01, 1.3600e-01, 2.6228e-01, 1.2090e-01, 1.2034e-01,\n",
      "          4.4586e-01, 3.8538e-01, 1.8873e-01, 3.3714e-01, 3.0419e-01,\n",
      "          4.5330e-01, 3.7225e-01, 2.7184e-01, 1.2500e-01, 2.5389e-01,\n",
      "          1.3373e-01, 1.2218e-01, 1.3127e-01, 1.2519e-01, 1.2633e-01,\n",
      "          3.6428e-01, 1.4466e-01, 1.2139e-01, 1.9580e-01, 1.9641e-01,\n",
      "          1.3601e-01, 1.3517e-01, 1.2224e-01, 1.2138e-01, 3.2443e-01,\n",
      "          2.3177e-01, 3.6858e-01, 1.6357e-01, 1.5412e-01, 1.4193e-01,\n",
      "          1.2213e-01, 3.3066e-01, 3.4196e-01, 4.2308e-01],\n",
      "         [1.1516e-01, 1.2181e-01, 1.2798e-02, 3.6507e-02, 2.5217e-01,\n",
      "          2.3329e-01, 0.0000e+00, 1.9338e-01, 1.3302e-01, 5.7290e-02,\n",
      "          1.6729e-01, 1.3223e-02, 3.2477e-01, 3.5725e-01, 1.9784e-01,\n",
      "          3.2925e-01, 4.9741e-02, 9.1397e-02, 0.0000e+00, 1.9098e-01,\n",
      "          1.6252e-01, 0.0000e+00, 7.7401e-02, 0.0000e+00, 3.4941e-02,\n",
      "          8.7024e-02, 1.6690e-01, 2.1034e-01, 0.0000e+00, 0.0000e+00,\n",
      "          1.0970e-01, 9.9579e-02, 2.4466e-01, 5.4579e-02, 3.1041e-01,\n",
      "          1.9205e-01, 1.7064e-01, 1.0794e-01, 1.5701e-01, 2.9401e-02,\n",
      "          1.8851e-01, 0.0000e+00, 0.0000e+00, 5.6341e-03, 0.0000e+00,\n",
      "          6.3943e-03, 1.7416e-01, 0.0000e+00, 0.0000e+00, 2.7203e-01,\n",
      "          5.2942e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          1.2595e-01, 1.0578e-01, 3.7711e-01, 2.2180e-01, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 1.5442e-01, 1.1925e-01]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[[1.2724e-01, 1.3042e-01, 1.2499e-01, 1.2213e-01, 1.2892e-01,\n",
      "          1.2497e-01, 1.2299e-01, 1.2887e-01, 1.3320e-01, 1.2860e-01,\n",
      "          1.2194e-01, 1.2663e-01, 1.3064e-01, 1.2907e-01, 1.3326e-01,\n",
      "          1.2605e-01, 1.2753e-01, 1.2448e-01, 1.2234e-01, 1.2780e-01,\n",
      "          1.2680e-01, 1.2655e-01, 1.2480e-01, 1.2377e-01, 1.2628e-01,\n",
      "          1.2578e-01, 1.2651e-01, 1.3026e-01, 1.2309e-01, 1.2236e-01,\n",
      "          1.3030e-01, 1.3073e-01, 1.2706e-01, 1.2964e-01, 1.2691e-01,\n",
      "          1.2899e-01, 1.2656e-01, 1.2789e-01, 1.2416e-01, 1.2621e-01,\n",
      "          1.2698e-01, 1.2306e-01, 1.2660e-01, 1.2576e-01, 1.2408e-01,\n",
      "          1.2783e-01, 1.3115e-01, 1.2235e-01, 1.2854e-01, 1.3131e-01,\n",
      "          1.8056e-01, 1.2790e-01, 1.2392e-01, 1.2232e-01, 1.2523e-01,\n",
      "          1.2232e-01, 1.2673e-01, 1.3046e-01, 1.2785e-01, 1.2703e-01,\n",
      "          1.2386e-01, 1.2571e-01, 1.2732e-01, 1.2797e-01],\n",
      "         [2.0045e-01, 2.3931e-01, 5.9981e-02, 5.4824e-02, 2.6505e-01,\n",
      "          2.2149e-01, 5.6714e-02, 2.4490e-01, 2.8843e-01, 6.2313e-02,\n",
      "          5.6474e-02, 3.4543e-01, 5.9946e-01, 2.9087e-01, 1.2073e-01,\n",
      "          1.3041e-01, 1.3943e-01, 2.5910e-01, 1.3538e-01, 6.2648e-02,\n",
      "          1.0265e-01, 6.2507e-02, 1.3034e-01, 6.0343e-02, 6.2294e-02,\n",
      "          5.7130e-02, 6.7583e-02, 2.3797e-01, 5.4327e-02, 5.4759e-02,\n",
      "          2.4256e-01, 2.0042e-01, 1.5531e-01, 1.3147e-01, 1.5780e-01,\n",
      "          3.3409e-01, 2.7020e-01, 2.5366e-01, 5.7717e-02, 7.2588e-02,\n",
      "          1.6795e-01, 5.5931e-02, 6.1665e-02, 7.6307e-02, 5.9453e-02,\n",
      "          1.4869e-01, 2.8620e-01, 5.6043e-02, 6.9391e-02, 3.2920e-01,\n",
      "          6.7947e-02, 6.4467e-02, 5.4701e-02, 5.6081e-02, 1.7513e-01,\n",
      "          2.2398e-01, 2.4988e-01, 2.1744e-01, 7.9651e-02, 6.0817e-02,\n",
      "          1.6217e-01, 9.2248e-02, 2.4892e-01, 3.5073e-01],\n",
      "         [8.1187e-03, 1.0766e-03, 0.0000e+00, 5.8896e-02, 2.8386e-02,\n",
      "          5.3435e-04, 5.5682e-02, 5.5261e-02, 0.0000e+00, 0.0000e+00,\n",
      "          1.9714e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          1.6839e-01, 1.9305e-01, 3.9559e-02, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 1.9076e-01, 2.0205e-01, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 2.4064e-02, 3.3992e-01, 1.6668e-02, 1.1209e-01,\n",
      "          0.0000e+00, 0.0000e+00, 8.9351e-02, 0.0000e+00, 0.0000e+00,\n",
      "          8.7057e-02, 0.0000e+00, 3.8820e-02, 0.0000e+00, 8.5093e-02,\n",
      "          1.2630e-01, 5.3903e-02, 7.8911e-02, 0.0000e+00, 0.0000e+00,\n",
      "          1.4795e-01, 0.0000e+00, 0.0000e+00, 1.6302e-01, 0.0000e+00,\n",
      "          0.0000e+00, 2.7825e-01, 5.0158e-02, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 5.7618e-02, 1.4594e-01,\n",
      "          1.6548e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [1.0385e-01, 1.0174e-01, 1.0174e-01, 1.1094e-01, 1.0023e-01,\n",
      "          1.0223e-01, 1.0461e-01, 1.0312e-01, 1.0052e-01, 1.0168e-01,\n",
      "          1.8719e-01, 1.0983e-01, 1.0211e-01, 1.0252e-01, 9.9416e-02,\n",
      "          1.0289e-01, 1.0646e-01, 1.0481e-01, 1.2426e-01, 1.0341e-01,\n",
      "          1.0370e-01, 1.0406e-01, 1.0436e-01, 1.0416e-01, 1.0385e-01,\n",
      "          1.0419e-01, 1.1850e-01, 1.0245e-01, 1.0381e-01, 1.0334e-01,\n",
      "          1.0114e-01, 1.0043e-01, 1.0171e-01, 1.0630e-01, 1.4707e-01,\n",
      "          1.0593e-01, 1.0390e-01, 1.0223e-01, 1.0350e-01, 1.0348e-01,\n",
      "          1.0375e-01, 1.0524e-01, 1.0487e-01, 1.0279e-01, 1.1186e-01,\n",
      "          1.0462e-01, 1.0222e-01, 1.3932e-01, 1.0441e-01, 1.0342e-01,\n",
      "          1.1699e-01, 1.0363e-01, 1.7849e-01, 1.0618e-01, 1.2266e-01,\n",
      "          1.0391e-01, 1.0412e-01, 1.0216e-01, 1.0285e-01, 1.0366e-01,\n",
      "          1.1337e-01, 1.9225e-01, 1.0362e-01, 1.0861e-01],\n",
      "         [5.0097e-01, 3.7297e-01, 4.5070e-01, 7.4322e-01, 5.4776e-01,\n",
      "          2.8396e-01, 4.9938e-01, 3.1183e-01, 1.8124e-01, 4.2878e-01,\n",
      "          8.9635e-01, 7.0047e-01, 4.8651e-01, 2.8486e-01, 3.4653e-01,\n",
      "          6.9069e-01, 6.6437e-01, 4.6877e-01, 6.1787e-01, 2.6410e-01,\n",
      "          3.5072e-01, 3.1867e-01, 3.5293e-01, 2.6831e-01, 3.0969e-01,\n",
      "          3.7887e-01, 3.5205e-01, 2.1147e-01, 4.1927e-01, 4.4680e-01,\n",
      "          1.7823e-01, 2.2934e-01, 4.6615e-01, 6.1075e-01, 7.2229e-01,\n",
      "          6.8554e-01, 5.5783e-01, 5.4806e-01, 4.1744e-01, 5.0782e-01,\n",
      "          3.0304e-01, 3.7942e-01, 2.3944e-01, 3.8023e-01, 3.5980e-01,\n",
      "          3.6684e-01, 1.7649e-01, 3.5553e-01, 3.7131e-01, 2.6636e-01,\n",
      "          2.2545e-01, 3.4879e-01, 6.7495e-01, 5.2387e-01, 5.3977e-01,\n",
      "          4.3467e-01, 4.3079e-01, 2.4674e-01, 3.0087e-01, 5.0491e-01,\n",
      "          7.4702e-01, 5.6256e-01, 4.5939e-01, 7.5032e-01],\n",
      "         [1.1645e-01, 1.2332e-01, 2.3066e-01, 2.5814e-01, 2.5580e-01,\n",
      "          5.3824e-02, 1.4940e-01, 0.0000e+00, 2.1549e-01, 2.4189e-01,\n",
      "          2.7610e-01, 5.1614e-01, 0.0000e+00, 1.5953e-01, 4.0866e-01,\n",
      "          1.1207e-01, 4.5554e-01, 2.8415e-02, 8.2100e-02, 1.0973e-01,\n",
      "          1.6314e-01, 1.5002e-01, 0.0000e+00, 1.6088e-01, 7.6305e-02,\n",
      "          5.5042e-02, 3.5174e-01, 1.5145e-01, 0.0000e+00, 5.6102e-02,\n",
      "          3.2858e-01, 2.0419e-01, 1.6849e-01, 4.4730e-01, 3.7297e-01,\n",
      "          3.7700e-01, 2.0079e-01, 9.9678e-02, 1.0282e-01, 3.1538e-01,\n",
      "          0.0000e+00, 6.3894e-02, 1.1134e-01, 0.0000e+00, 1.7802e-01,\n",
      "          3.0735e-01, 0.0000e+00, 6.3166e-02, 3.8274e-01, 0.0000e+00,\n",
      "          3.0821e-01, 1.8192e-01, 0.0000e+00, 1.9800e-01, 1.7150e-01,\n",
      "          2.4561e-02, 1.5689e-01, 0.0000e+00, 1.6918e-01, 2.1959e-01,\n",
      "          0.0000e+00, 3.1750e-01, 2.1306e-01, 1.3323e-01],\n",
      "         [2.9143e-01, 2.8436e-01, 1.6929e-01, 1.2012e-01, 3.7550e-01,\n",
      "          2.1271e-01, 1.7201e-01, 2.1496e-01, 4.6608e-01, 1.3427e-01,\n",
      "          1.2122e-01, 6.1367e-01, 5.5929e-01, 3.8907e-01, 2.9982e-01,\n",
      "          1.2944e-01, 3.6019e-01, 2.2364e-01, 2.3510e-01, 1.3356e-01,\n",
      "          1.9990e-01, 1.3189e-01, 1.2818e-01, 1.2667e-01, 1.5611e-01,\n",
      "          1.2653e-01, 1.3600e-01, 2.6228e-01, 1.2090e-01, 1.2034e-01,\n",
      "          4.4586e-01, 3.8538e-01, 1.8873e-01, 3.3714e-01, 3.0419e-01,\n",
      "          4.5330e-01, 3.7225e-01, 2.7184e-01, 1.2500e-01, 2.5389e-01,\n",
      "          1.3373e-01, 1.2218e-01, 1.3127e-01, 1.2519e-01, 1.2633e-01,\n",
      "          3.6428e-01, 1.4466e-01, 1.2139e-01, 1.9580e-01, 1.9641e-01,\n",
      "          1.3601e-01, 1.3517e-01, 1.2224e-01, 1.2138e-01, 3.2443e-01,\n",
      "          2.3177e-01, 3.6858e-01, 1.6357e-01, 1.5412e-01, 1.4193e-01,\n",
      "          1.2213e-01, 3.3066e-01, 3.4196e-01, 4.2308e-01],\n",
      "         [1.1516e-01, 1.2181e-01, 1.2798e-02, 3.6507e-02, 2.5217e-01,\n",
      "          2.3329e-01, 0.0000e+00, 1.9338e-01, 1.3302e-01, 5.7290e-02,\n",
      "          1.6729e-01, 1.3223e-02, 3.2477e-01, 3.5725e-01, 1.9784e-01,\n",
      "          3.2925e-01, 4.9741e-02, 9.1397e-02, 0.0000e+00, 1.9098e-01,\n",
      "          1.6252e-01, 0.0000e+00, 7.7401e-02, 0.0000e+00, 3.4941e-02,\n",
      "          8.7024e-02, 1.6690e-01, 2.1034e-01, 0.0000e+00, 0.0000e+00,\n",
      "          1.0970e-01, 9.9579e-02, 2.4466e-01, 5.4579e-02, 3.1041e-01,\n",
      "          1.9205e-01, 1.7064e-01, 1.0794e-01, 1.5701e-01, 2.9401e-02,\n",
      "          1.8851e-01, 0.0000e+00, 0.0000e+00, 5.6341e-03, 0.0000e+00,\n",
      "          6.3943e-03, 1.7416e-01, 0.0000e+00, 0.0000e+00, 2.7203e-01,\n",
      "          5.2942e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          1.2595e-01, 1.0578e-01, 3.7711e-01, 2.2180e-01, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 1.5442e-01, 1.1925e-01]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[[0.2769, 0.2902, 0.2898, 0.2661, 0.2743, 0.2778, 0.2852, 0.2897,\n",
      "          0.2967, 0.2837, 0.2777, 0.2857, 0.2833, 0.2894, 0.2924, 0.2735,\n",
      "          0.2760, 0.2798, 0.2811, 0.2877, 0.2889, 0.2815, 0.2747, 0.2790,\n",
      "          0.2876, 0.2918, 0.2863, 0.2861, 0.2813, 0.2784, 0.3012, 0.2983,\n",
      "          0.2837, 0.2897, 0.2824, 0.2840, 0.2845, 0.2817, 0.2802, 0.2818,\n",
      "          0.2790, 0.2722, 0.2833, 0.2861, 0.2844, 0.2884, 0.2882, 0.2795,\n",
      "          0.2919, 0.2924, 0.3388, 0.2859, 0.2829, 0.2767, 0.2873, 0.2891,\n",
      "          0.2886, 0.2883, 0.2826, 0.2854, 0.2697, 0.2831, 0.2973, 0.2859],\n",
      "         [0.2004, 0.2393, 0.0600, 0.0548, 0.2651, 0.2215, 0.0567, 0.2449,\n",
      "          0.2884, 0.0623, 0.0565, 0.3454, 0.5995, 0.2909, 0.1207, 0.1304,\n",
      "          0.1394, 0.2591, 0.1354, 0.0626, 0.1027, 0.0625, 0.1303, 0.0603,\n",
      "          0.0623, 0.0571, 0.0676, 0.2380, 0.0543, 0.0548, 0.2426, 0.2004,\n",
      "          0.1553, 0.1315, 0.1578, 0.3341, 0.2702, 0.2537, 0.0577, 0.0726,\n",
      "          0.1679, 0.0559, 0.0617, 0.0763, 0.0595, 0.1487, 0.2862, 0.0560,\n",
      "          0.0694, 0.3292, 0.0679, 0.0645, 0.0547, 0.0561, 0.1751, 0.2240,\n",
      "          0.2499, 0.2174, 0.0797, 0.0608, 0.1622, 0.0922, 0.2489, 0.3507],\n",
      "         [0.1012, 0.0792, 0.0911, 0.1634, 0.1182, 0.0961, 0.1519, 0.1322,\n",
      "          0.0702, 0.0945, 0.3063, 0.0936, 0.0941, 0.0883, 0.0746, 0.2772,\n",
      "          0.2891, 0.1316, 0.0937, 0.0832, 0.0892, 0.2856, 0.2986, 0.0925,\n",
      "          0.0911, 0.0943, 0.1147, 0.4350, 0.1134, 0.2072, 0.0881, 0.0900,\n",
      "          0.1848, 0.0915, 0.1109, 0.1873, 0.0880, 0.1301, 0.0875, 0.1775,\n",
      "          0.2206, 0.1445, 0.1664, 0.0902, 0.0914, 0.2365, 0.0901, 0.0921,\n",
      "          0.2597, 0.0953, 0.0940, 0.3746, 0.1477, 0.0966, 0.0942, 0.0915,\n",
      "          0.0796, 0.0915, 0.1536, 0.2357, 0.1149, 0.0922, 0.0880, 0.0886],\n",
      "         [0.1656, 0.1457, 0.1443, 0.1802, 0.1657, 0.1592, 0.1518, 0.1502,\n",
      "          0.1435, 0.1561, 0.2457, 0.1612, 0.1802, 0.1511, 0.1438, 0.1746,\n",
      "          0.1704, 0.1640, 0.1767, 0.1483, 0.1487, 0.1583, 0.1716, 0.1586,\n",
      "          0.1509, 0.1474, 0.1671, 0.1707, 0.1555, 0.1580, 0.1379, 0.1425,\n",
      "          0.1546, 0.1547, 0.2067, 0.1690, 0.1558, 0.1625, 0.1535, 0.1560,\n",
      "          0.1649, 0.1639, 0.1562, 0.1505, 0.1594, 0.1532, 0.1541, 0.1899,\n",
      "          0.1515, 0.1557, 0.1687, 0.1603, 0.2330, 0.1611, 0.1708, 0.1486,\n",
      "          0.1482, 0.1521, 0.1578, 0.1560, 0.1835, 0.2434, 0.1409, 0.1686],\n",
      "         [0.5010, 0.3730, 0.4507, 0.7432, 0.5478, 0.2840, 0.4994, 0.3118,\n",
      "          0.1812, 0.4288, 0.8964, 0.7005, 0.4865, 0.2849, 0.3465, 0.6907,\n",
      "          0.6644, 0.4688, 0.6179, 0.2641, 0.3507, 0.3187, 0.3529, 0.2683,\n",
      "          0.3097, 0.3789, 0.3521, 0.2115, 0.4193, 0.4468, 0.1782, 0.2293,\n",
      "          0.4662, 0.6108, 0.7223, 0.6855, 0.5578, 0.5481, 0.4174, 0.5078,\n",
      "          0.3030, 0.3794, 0.2394, 0.3802, 0.3598, 0.3668, 0.1765, 0.3555,\n",
      "          0.3713, 0.2664, 0.2255, 0.3488, 0.6750, 0.5239, 0.5398, 0.4347,\n",
      "          0.4308, 0.2467, 0.3009, 0.5049, 0.7470, 0.5626, 0.4594, 0.7503],\n",
      "         [0.1264, 0.1457, 0.2391, 0.2581, 0.2697, 0.0592, 0.1494, 0.0259,\n",
      "          0.2411, 0.2476, 0.2773, 0.5241, 0.0210, 0.1706, 0.4232, 0.1121,\n",
      "          0.4633, 0.0414, 0.0886, 0.1274, 0.1738, 0.1567, 0.0117, 0.1676,\n",
      "          0.0841, 0.0584, 0.3604, 0.1626, 0.0029, 0.0618, 0.3368, 0.2093,\n",
      "          0.1733, 0.4552, 0.3730, 0.3804, 0.2158, 0.1135, 0.1178, 0.3244,\n",
      "          0.0106, 0.0763, 0.1232, 0.0092, 0.1866, 0.3178, 0.0088, 0.0725,\n",
      "          0.3827, 0.0046, 0.3129, 0.1866, 0.0040, 0.2031, 0.1757, 0.0327,\n",
      "          0.1763, 0.0093, 0.1739, 0.2277, 0.0088, 0.3255, 0.2213, 0.1542],\n",
      "         [0.2914, 0.2844, 0.1693, 0.1201, 0.3755, 0.2127, 0.1720, 0.2150,\n",
      "          0.4661, 0.1343, 0.1212, 0.6137, 0.5593, 0.3891, 0.2998, 0.1294,\n",
      "          0.3602, 0.2236, 0.2351, 0.1336, 0.1999, 0.1319, 0.1282, 0.1267,\n",
      "          0.1561, 0.1265, 0.1360, 0.2623, 0.1209, 0.1203, 0.4459, 0.3854,\n",
      "          0.1887, 0.3371, 0.3042, 0.4533, 0.3723, 0.2718, 0.1250, 0.2539,\n",
      "          0.1337, 0.1222, 0.1313, 0.1252, 0.1263, 0.3643, 0.1447, 0.1214,\n",
      "          0.1958, 0.1964, 0.1360, 0.1352, 0.1222, 0.1214, 0.3244, 0.2318,\n",
      "          0.3686, 0.1636, 0.1541, 0.1419, 0.1221, 0.3307, 0.3420, 0.4231],\n",
      "         [0.1152, 0.1218, 0.0128, 0.0365, 0.2522, 0.2333, 0.0000, 0.1934,\n",
      "          0.1330, 0.0573, 0.1673, 0.0132, 0.3248, 0.3573, 0.1978, 0.3293,\n",
      "          0.0497, 0.0914, 0.0000, 0.1910, 0.1625, 0.0000, 0.0774, 0.0000,\n",
      "          0.0349, 0.0870, 0.1669, 0.2103, 0.0000, 0.0000, 0.1097, 0.0996,\n",
      "          0.2447, 0.0546, 0.3104, 0.1920, 0.1706, 0.1079, 0.1570, 0.0294,\n",
      "          0.1885, 0.0000, 0.0000, 0.0056, 0.0000, 0.0064, 0.1742, 0.0000,\n",
      "          0.0000, 0.2720, 0.0529, 0.0000, 0.0000, 0.0000, 0.0000, 0.1259,\n",
      "          0.1058, 0.3771, 0.2218, 0.0000, 0.0000, 0.0000, 0.1544, 0.1193]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# x = model(input_feats, lengths = None)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m y\n",
      "File \u001b[1;32mc:\\Users\\chiny\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\chiny\\OneDrive - Nanyang Technological University\\Ad-Hoc Internships\\AY24 DSO Summer\\LegacyCNN\\python_lib\\ecapa_classifier.py:596\u001b[0m, in \u001b[0;36mECAPA_TDNN.forward\u001b[1;34m(self, x, lengths)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;66;03m# Attentive Statistical Pooling\u001b[39;00m\n\u001b[0;32m    595\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masp(x, lengths\u001b[38;5;241m=\u001b[39mlengths)\n\u001b[1;32m--> 596\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masp_bn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;66;03m# Final linear transformation\u001b[39;00m\n\u001b[0;32m    599\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n",
      "File \u001b[1;32mc:\\Users\\chiny\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\chiny\\anaconda3\\envs\\pytorch\\lib\\site-packages\\speechbrain\\nnet\\normalization.py:81\u001b[0m, in \u001b[0;36mBatchNorm1d.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     73\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the normalized input tensor.\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03m    Arguments\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;124;03m        4d tensors can be used when combine_dims=True.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m     shape_or \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_batch_time:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    " \n",
    "# x = model(input_feats, lengths = None)\n",
    "x= model(input_feats, lengths = torch.ones((1)) * 0.4)\n",
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
